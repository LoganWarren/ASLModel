{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![handalp.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "pRAyYsBGeC9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmJisH06ARdo",
        "outputId": "182c2dff-146d-4225-b9ca-885bb6fb6023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-decision-forests>=1.0.1\n",
            "  Downloading tensorflow_decision_forests-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (0.4.6)\n",
            "Collecting packaging~=20.9\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (3.19.6)\n",
            "Requirement already satisfied: tensorflow<3,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (2.11.0)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (5.12.0)\n",
            "Requirement already satisfied: flax>=0.6.2 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (0.6.7)\n",
            "Collecting tensorflow-hub<0.13,>=0.7.0\n",
            "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (1.0.5)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.33)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (13.3.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (1.22.4)\n",
            "Requirement already satisfied: orbax in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib_resources>=5.9.0->tensorflowjs) (3.15.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.16->tensorflowjs) (1.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.31.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.51.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.11.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (67.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.4.0)\n",
            "Collecting tensorflow<3,>=2.10.0\n",
            "  Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from tensorflow-decision-forests>=1.0.1->tensorflowjs) (0.40.0)\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from tensorflow-decision-forests>=1.0.1->tensorflowjs) (1.4.4)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-decision-forests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-decision-forests>=1.0.1\n",
            "  Downloading tensorflow_decision_forests-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax>=0.6.2->tensorflowjs) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax>=0.6.2->tensorflowjs) (2.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax->flax>=0.6.2->tensorflowjs) (0.4.6+cuda11.cudnn86)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax->flax>=0.6.2->tensorflowjs) (0.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.5.6)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->tensorflow-decision-forests>=1.0.1->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->tensorflow-decision-forests>=1.0.1->tensorflowjs) (2022.7.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax>=0.6.2->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax>=0.6.2->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (6.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax>=0.6.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.2.2)\n",
            "Installing collected packages: wurlitzer, tensorflow-hub, packaging, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.13.0\n",
            "    Uninstalling tensorflow-hub-0.13.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.13.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-20.9 tensorflow-decision-forests-1.2.0 tensorflow-hub-0.12.0 tensorflowjs-4.3.0 wurlitzer-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwAsvJHDm6Ho",
        "outputId": "9b7b0ccf-90ac-415c-807d-e659f0da615f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "import csv\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CSV file from the provided link\n",
        "url1 = 'https://raw.githubusercontent.com/Vaxier8/vaxier8.github.io/main/trainFinalNum.csv'\n",
        "url2 = 'https://raw.githubusercontent.com/Vaxier8/vaxier8.github.io/main/test.csv'\n",
        "response1 = requests.get(url1)\n",
        "response2 = requests.get(url2)\n",
        "\n",
        "# Read the CSV file and remove lines with more than 43 fields and exclude rows with missing values\n",
        "rows = csv.reader(response1.text.strip().split('\\n'))\n",
        "cleaned_rows = []\n",
        "for row in rows:\n",
        "    if len(row) <= 43 and all(row):\n",
        "        cleaned_rows.append(row)\n",
        "\n",
        "\n",
        "# Write the cleaned data to a new file\n",
        "with open('trainFinal_cleaned.csv', 'w', newline='') as outfile:\n",
        "    writer = csv.writer(outfile)\n",
        "    writer.writerows(cleaned_rows)\n",
        "\n",
        "# Read the cleaned data into a pandas DataFrame object\n",
        "abalone_train = pd.read_csv('trainFinal_cleaned.csv', names=[\"label\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"X3\", \"Y3\", \"X4\", \"Y4\", \"X5\", \"Y5\", \"X6\", \"Y6\", \"X7\", \"Y7\", \"X8\", \"Y8\", \"X9\", \"Y9\", \"X10\", \"Y10\", \"X11\", \"Y11\", \"X12\", \"Y12\", \"X13\", \"Y13\", \"X14\", \"Y14\", \"X15\", \"Y15\", \"X16\", \"Y16\", \"X17\", \"Y17\", \"X18\", \"Y18\", \"X19\", \"Y19\", \"X20\", \"Y20\", \"X21\", \"Y21\"])\n",
        "y_train = abalone_train.pop(\"label\")\n",
        "\n",
        "x_test = pd.read_csv(url2, names=[\"label\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"X3\", \"Y3\", \"X4\", \"Y4\", \"X5\", \"Y5\", \"X6\", \"Y6\", \"X7\", \"Y7\", \"X8\", \"Y8\", \"X9\", \"Y9\", \"X10\", \"Y10\", \"X11\", \"Y11\", \"X12\", \"Y12\", \"X13\", \"Y13\", \"X14\", \"Y14\", \"X15\", \"Y15\", \"X16\", \"Y16\", \"X17\", \"Y17\", \"X18\", \"Y18\", \"X19\", \"Y19\", \"X20\", \"Y20\", \"X21\", \"Y21\"])\n",
        "y_test = x_test.pop(\"label\")\n",
        "\n",
        "tf.convert_to_tensor(abalone_train, dtype=tf.float32)\n",
        "tf.convert_to_tensor(x_test, dtype=tf.float32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwbUACcgnFYb",
        "outputId": "cee4801e-17ec-4ba1-cc6b-748b4e9ee5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 42), dtype=float32, numpy=\n",
              "array([[0.477, 0.632, 0.419, 0.641, 0.378, 0.666, 0.355, 0.689, 0.342,\n",
              "        0.688, 0.361, 0.617, 0.357, 0.62 , 0.338, 0.618, 0.319, 0.613,\n",
              "        0.398, 0.607, 0.372, 0.734, 0.365, 0.809, 0.365, 0.859, 0.444,\n",
              "        0.629, 0.415, 0.757, 0.426, 0.757, 0.441, 0.731, 0.483, 0.657,\n",
              "        0.457, 0.766, 0.459, 0.77 , 0.465, 0.743],\n",
              "       [0.469, 0.634, 0.408, 0.648, 0.368, 0.674, 0.347, 0.696, 0.341,\n",
              "        0.691, 0.354, 0.62 , 0.356, 0.619, 0.338, 0.618, 0.319, 0.616,\n",
              "        0.394, 0.607, 0.372, 0.731, 0.363, 0.801, 0.361, 0.847, 0.443,\n",
              "        0.625, 0.415, 0.755, 0.425, 0.752, 0.441, 0.72 , 0.483, 0.652,\n",
              "        0.458, 0.761, 0.459, 0.762, 0.466, 0.73 ],\n",
              "       [0.477, 0.609, 0.432, 0.693, 0.352, 0.755, 0.293, 0.802, 0.261,\n",
              "        0.861, 0.292, 0.678, 0.215, 0.684, 0.243, 0.685, 0.285, 0.684,\n",
              "        0.289, 0.619, 0.203, 0.61 , 0.241, 0.622, 0.286, 0.63 , 0.298,\n",
              "        0.56 , 0.215, 0.541, 0.241, 0.563, 0.282, 0.577, 0.316, 0.5  ,\n",
              "        0.251, 0.482, 0.254, 0.502, 0.278, 0.519],\n",
              "       [0.555, 0.786, 0.493, 0.724, 0.495, 0.608, 0.558, 0.545, 0.603,\n",
              "        0.495, 0.488, 0.543, 0.474, 0.462, 0.478, 0.526, 0.488, 0.592,\n",
              "        0.533, 0.532, 0.511, 0.427, 0.508, 0.496, 0.517, 0.565, 0.579,\n",
              "        0.543, 0.567, 0.423, 0.558, 0.488, 0.562, 0.551, 0.622, 0.571,\n",
              "        0.581, 0.54 , 0.567, 0.605, 0.573, 0.653],\n",
              "       [0.412, 1.009, 0.34 , 0.977, 0.286, 0.873, 0.292, 0.771, 0.34 ,\n",
              "        0.724, 0.287, 0.768, 0.268, 0.689, 0.289, 0.762, 0.304, 0.802,\n",
              "        0.329, 0.744, 0.314, 0.666, 0.33 , 0.759, 0.326, 0.764, 0.372,\n",
              "        0.737, 0.362, 0.653, 0.369, 0.751, 0.376, 0.811, 0.42 , 0.738,\n",
              "        0.425, 0.643, 0.429, 0.582, 0.429, 0.514],\n",
              "       [0.414, 1.01 , 0.343, 0.979, 0.286, 0.873, 0.29 , 0.768, 0.341,\n",
              "        0.723, 0.286, 0.765, 0.268, 0.688, 0.29 , 0.761, 0.306, 0.797,\n",
              "        0.328, 0.742, 0.315, 0.664, 0.33 , 0.755, 0.33 , 0.769, 0.372,\n",
              "        0.735, 0.363, 0.653, 0.369, 0.75 , 0.375, 0.809, 0.421, 0.735,\n",
              "        0.426, 0.639, 0.429, 0.577, 0.428, 0.508],\n",
              "       [0.563, 0.679, 0.505, 0.648, 0.455, 0.571, 0.441, 0.496, 0.461,\n",
              "        0.448, 0.488, 0.5  , 0.483, 0.456, 0.491, 0.518, 0.497, 0.537,\n",
              "        0.523, 0.493, 0.516, 0.445, 0.521, 0.522, 0.528, 0.537, 0.559,\n",
              "        0.498, 0.556, 0.459, 0.554, 0.532, 0.556, 0.543, 0.597, 0.51 ,\n",
              "        0.59 , 0.486, 0.583, 0.536, 0.583, 0.544],\n",
              "       [0.457, 0.673, 0.401, 0.631, 0.373, 0.531, 0.397, 0.455, 0.439,\n",
              "        0.44 , 0.399, 0.44 , 0.4  , 0.352, 0.405, 0.297, 0.412, 0.248,\n",
              "        0.435, 0.428, 0.434, 0.331, 0.438, 0.27 , 0.441, 0.216, 0.467,\n",
              "        0.437, 0.468, 0.347, 0.47 , 0.289, 0.471, 0.239, 0.499, 0.462,\n",
              "        0.499, 0.391, 0.501, 0.346, 0.502, 0.305]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    tf.keras.layers.Input((21 * 2, )),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(40, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(30, activation='relu'),\n",
        "    tf.keras.layers.Dense(29, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "_1p6G5aNnS-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MazgKPMBouxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"training/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
        "\n",
        "model.fit(\n",
        "    abalone_train,\n",
        "    y_train,\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[cp_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii6Ljgbbpwmy",
        "outputId": "e5dc6d4d-d098-4f33-8b4d-d9f01b8403d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "202/202 [==============================] - 2s 4ms/step - loss: 3.2825 - accuracy: 0.0778 - val_loss: 3.2263 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 3.1456 - accuracy: 0.1034 - val_loss: 3.0153 - val_accuracy: 0.1250\n",
            "Epoch 3/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 3.0034 - accuracy: 0.1413 - val_loss: 2.7254 - val_accuracy: 0.1250\n",
            "Epoch 4/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.8779 - accuracy: 0.1638 - val_loss: 2.5219 - val_accuracy: 0.5000\n",
            "Epoch 5/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.7937 - accuracy: 0.1878 - val_loss: 2.3636 - val_accuracy: 0.3750\n",
            "Epoch 6/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.7316 - accuracy: 0.1983 - val_loss: 2.2473 - val_accuracy: 0.7500\n",
            "Epoch 7/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 2.6742 - accuracy: 0.2156 - val_loss: 2.1201 - val_accuracy: 0.6250\n",
            "Epoch 8/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.6211 - accuracy: 0.2223 - val_loss: 2.1256 - val_accuracy: 0.3750\n",
            "Epoch 9/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.5674 - accuracy: 0.2389 - val_loss: 2.1142 - val_accuracy: 0.7500\n",
            "Epoch 10/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.5140 - accuracy: 0.2514 - val_loss: 2.0053 - val_accuracy: 0.5000\n",
            "Epoch 11/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.4429 - accuracy: 0.2742 - val_loss: 2.1708 - val_accuracy: 0.5000\n",
            "Epoch 12/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.3645 - accuracy: 0.2936 - val_loss: 2.0293 - val_accuracy: 0.5000\n",
            "Epoch 13/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 2.2912 - accuracy: 0.3083 - val_loss: 2.1117 - val_accuracy: 0.3750\n",
            "Epoch 14/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 2.2324 - accuracy: 0.3247 - val_loss: 2.1207 - val_accuracy: 0.3750\n",
            "Epoch 15/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 2.1577 - accuracy: 0.3473 - val_loss: 2.1405 - val_accuracy: 0.5000\n",
            "Epoch 16/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 2.1108 - accuracy: 0.3589 - val_loss: 2.0941 - val_accuracy: 0.5000\n",
            "Epoch 17/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 2.0341 - accuracy: 0.3770 - val_loss: 2.1501 - val_accuracy: 0.5000\n",
            "Epoch 18/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.9791 - accuracy: 0.3923 - val_loss: 2.1153 - val_accuracy: 0.5000\n",
            "Epoch 19/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.9188 - accuracy: 0.4088 - val_loss: 2.1360 - val_accuracy: 0.2500\n",
            "Epoch 20/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.8704 - accuracy: 0.4237 - val_loss: 2.0690 - val_accuracy: 0.3750\n",
            "Epoch 21/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.8144 - accuracy: 0.4368 - val_loss: 2.2229 - val_accuracy: 0.1250\n",
            "Epoch 22/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.7584 - accuracy: 0.4542 - val_loss: 2.2065 - val_accuracy: 0.1250\n",
            "Epoch 23/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.7262 - accuracy: 0.4648 - val_loss: 2.1957 - val_accuracy: 0.1250\n",
            "Epoch 24/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.6864 - accuracy: 0.4740 - val_loss: 2.2301 - val_accuracy: 0.1250\n",
            "Epoch 25/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.6386 - accuracy: 0.4912 - val_loss: 2.3301 - val_accuracy: 0.1250\n",
            "Epoch 26/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.6163 - accuracy: 0.4957 - val_loss: 2.2650 - val_accuracy: 0.1250\n",
            "Epoch 27/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.5939 - accuracy: 0.5063 - val_loss: 2.4318 - val_accuracy: 0.1250\n",
            "Epoch 28/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.5704 - accuracy: 0.5082 - val_loss: 2.3710 - val_accuracy: 0.1250\n",
            "Epoch 29/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.5584 - accuracy: 0.5190 - val_loss: 2.3804 - val_accuracy: 0.1250\n",
            "Epoch 30/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.5405 - accuracy: 0.5223 - val_loss: 2.3860 - val_accuracy: 0.1250\n",
            "Epoch 31/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.5221 - accuracy: 0.5264 - val_loss: 2.4028 - val_accuracy: 0.1250\n",
            "Epoch 32/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.5009 - accuracy: 0.5337 - val_loss: 2.3645 - val_accuracy: 0.1250\n",
            "Epoch 33/1000\n",
            "202/202 [==============================] - 2s 8ms/step - loss: 1.4889 - accuracy: 0.5372 - val_loss: 2.2225 - val_accuracy: 0.1250\n",
            "Epoch 34/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.4561 - accuracy: 0.5429 - val_loss: 2.4466 - val_accuracy: 0.1250\n",
            "Epoch 35/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.4577 - accuracy: 0.5452 - val_loss: 2.4618 - val_accuracy: 0.1250\n",
            "Epoch 36/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4393 - accuracy: 0.5489 - val_loss: 2.3443 - val_accuracy: 0.1250\n",
            "Epoch 37/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4384 - accuracy: 0.5518 - val_loss: 2.3634 - val_accuracy: 0.1250\n",
            "Epoch 38/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4192 - accuracy: 0.5551 - val_loss: 2.3387 - val_accuracy: 0.1250\n",
            "Epoch 39/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4117 - accuracy: 0.5569 - val_loss: 2.4299 - val_accuracy: 0.1250\n",
            "Epoch 40/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4086 - accuracy: 0.5632 - val_loss: 2.4570 - val_accuracy: 0.1250\n",
            "Epoch 41/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3938 - accuracy: 0.5628 - val_loss: 2.5457 - val_accuracy: 0.1250\n",
            "Epoch 42/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3924 - accuracy: 0.5665 - val_loss: 2.6061 - val_accuracy: 0.1250\n",
            "Epoch 43/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.4012 - accuracy: 0.5629 - val_loss: 2.4562 - val_accuracy: 0.1250\n",
            "Epoch 44/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3756 - accuracy: 0.5736 - val_loss: 2.4215 - val_accuracy: 0.1250\n",
            "Epoch 45/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3688 - accuracy: 0.5729 - val_loss: 2.4825 - val_accuracy: 0.1250\n",
            "Epoch 46/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3709 - accuracy: 0.5736 - val_loss: 2.4733 - val_accuracy: 0.1250\n",
            "Epoch 47/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3607 - accuracy: 0.5744 - val_loss: 2.5363 - val_accuracy: 0.1250\n",
            "Epoch 48/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3558 - accuracy: 0.5783 - val_loss: 2.5448 - val_accuracy: 0.1250\n",
            "Epoch 49/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3583 - accuracy: 0.5787 - val_loss: 2.5603 - val_accuracy: 0.1250\n",
            "Epoch 50/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3467 - accuracy: 0.5820 - val_loss: 2.7162 - val_accuracy: 0.1250\n",
            "Epoch 51/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.3323 - accuracy: 0.5899 - val_loss: 2.4995 - val_accuracy: 0.1250\n",
            "Epoch 52/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.3412 - accuracy: 0.5860 - val_loss: 2.5561 - val_accuracy: 0.1250\n",
            "Epoch 53/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.3431 - accuracy: 0.5806 - val_loss: 2.4937 - val_accuracy: 0.1250\n",
            "Epoch 54/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.3191 - accuracy: 0.5916 - val_loss: 2.6574 - val_accuracy: 0.1250\n",
            "Epoch 55/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3177 - accuracy: 0.5887 - val_loss: 2.5755 - val_accuracy: 0.1250\n",
            "Epoch 56/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3202 - accuracy: 0.5902 - val_loss: 2.5367 - val_accuracy: 0.1250\n",
            "Epoch 57/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3062 - accuracy: 0.5934 - val_loss: 2.6467 - val_accuracy: 0.1250\n",
            "Epoch 58/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2994 - accuracy: 0.5945 - val_loss: 2.5574 - val_accuracy: 0.1250\n",
            "Epoch 59/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.3044 - accuracy: 0.5947 - val_loss: 2.4970 - val_accuracy: 0.1250\n",
            "Epoch 60/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2977 - accuracy: 0.5952 - val_loss: 2.5651 - val_accuracy: 0.1250\n",
            "Epoch 61/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2763 - accuracy: 0.5991 - val_loss: 2.5963 - val_accuracy: 0.1250\n",
            "Epoch 62/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2692 - accuracy: 0.6071 - val_loss: 2.6627 - val_accuracy: 0.1250\n",
            "Epoch 63/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2805 - accuracy: 0.6030 - val_loss: 2.6369 - val_accuracy: 0.1250\n",
            "Epoch 64/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2666 - accuracy: 0.6039 - val_loss: 2.6654 - val_accuracy: 0.1250\n",
            "Epoch 65/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2651 - accuracy: 0.6061 - val_loss: 2.6066 - val_accuracy: 0.1250\n",
            "Epoch 66/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2753 - accuracy: 0.6039 - val_loss: 2.5829 - val_accuracy: 0.1250\n",
            "Epoch 67/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2468 - accuracy: 0.6133 - val_loss: 2.5261 - val_accuracy: 0.1250\n",
            "Epoch 68/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2537 - accuracy: 0.6058 - val_loss: 2.6136 - val_accuracy: 0.1250\n",
            "Epoch 69/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2583 - accuracy: 0.6053 - val_loss: 2.6725 - val_accuracy: 0.1250\n",
            "Epoch 70/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.2472 - accuracy: 0.6137 - val_loss: 2.5905 - val_accuracy: 0.1250\n",
            "Epoch 71/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.2316 - accuracy: 0.6183 - val_loss: 2.5690 - val_accuracy: 0.1250\n",
            "Epoch 72/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.2399 - accuracy: 0.6149 - val_loss: 2.6889 - val_accuracy: 0.1250\n",
            "Epoch 73/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2341 - accuracy: 0.6167 - val_loss: 2.6728 - val_accuracy: 0.1250\n",
            "Epoch 74/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2277 - accuracy: 0.6190 - val_loss: 2.6903 - val_accuracy: 0.1250\n",
            "Epoch 75/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2206 - accuracy: 0.6172 - val_loss: 2.6468 - val_accuracy: 0.1250\n",
            "Epoch 76/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2323 - accuracy: 0.6170 - val_loss: 2.6884 - val_accuracy: 0.1250\n",
            "Epoch 77/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2237 - accuracy: 0.6218 - val_loss: 2.6396 - val_accuracy: 0.1250\n",
            "Epoch 78/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2238 - accuracy: 0.6216 - val_loss: 2.5503 - val_accuracy: 0.1250\n",
            "Epoch 79/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2239 - accuracy: 0.6208 - val_loss: 2.7161 - val_accuracy: 0.1250\n",
            "Epoch 80/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2194 - accuracy: 0.6179 - val_loss: 2.7169 - val_accuracy: 0.1250\n",
            "Epoch 81/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2080 - accuracy: 0.6250 - val_loss: 2.6257 - val_accuracy: 0.1250\n",
            "Epoch 82/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1972 - accuracy: 0.6297 - val_loss: 2.6591 - val_accuracy: 0.1250\n",
            "Epoch 83/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2290 - accuracy: 0.6165 - val_loss: 2.6624 - val_accuracy: 0.1250\n",
            "Epoch 84/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2024 - accuracy: 0.6217 - val_loss: 2.6922 - val_accuracy: 0.1250\n",
            "Epoch 85/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2067 - accuracy: 0.6219 - val_loss: 2.7480 - val_accuracy: 0.1250\n",
            "Epoch 86/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2078 - accuracy: 0.6204 - val_loss: 2.6753 - val_accuracy: 0.1250\n",
            "Epoch 87/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.2057 - accuracy: 0.6234 - val_loss: 2.6983 - val_accuracy: 0.1250\n",
            "Epoch 88/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.2088 - accuracy: 0.6204 - val_loss: 2.7316 - val_accuracy: 0.1250\n",
            "Epoch 89/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.2010 - accuracy: 0.6242 - val_loss: 2.6530 - val_accuracy: 0.1250\n",
            "Epoch 90/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1845 - accuracy: 0.6250 - val_loss: 2.7165 - val_accuracy: 0.1250\n",
            "Epoch 91/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1978 - accuracy: 0.6218 - val_loss: 2.7160 - val_accuracy: 0.1250\n",
            "Epoch 92/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1942 - accuracy: 0.6260 - val_loss: 2.7173 - val_accuracy: 0.1250\n",
            "Epoch 93/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1956 - accuracy: 0.6297 - val_loss: 2.7642 - val_accuracy: 0.1250\n",
            "Epoch 94/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1866 - accuracy: 0.6309 - val_loss: 2.7247 - val_accuracy: 0.1250\n",
            "Epoch 95/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1919 - accuracy: 0.6266 - val_loss: 2.6416 - val_accuracy: 0.1250\n",
            "Epoch 96/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1842 - accuracy: 0.6302 - val_loss: 2.8617 - val_accuracy: 0.1250\n",
            "Epoch 97/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1879 - accuracy: 0.6322 - val_loss: 2.6819 - val_accuracy: 0.1250\n",
            "Epoch 98/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1841 - accuracy: 0.6315 - val_loss: 2.7728 - val_accuracy: 0.1250\n",
            "Epoch 99/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1721 - accuracy: 0.6369 - val_loss: 2.7871 - val_accuracy: 0.1250\n",
            "Epoch 100/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1864 - accuracy: 0.6294 - val_loss: 2.6970 - val_accuracy: 0.1250\n",
            "Epoch 101/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1774 - accuracy: 0.6325 - val_loss: 2.7486 - val_accuracy: 0.1250\n",
            "Epoch 102/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1764 - accuracy: 0.6298 - val_loss: 2.8656 - val_accuracy: 0.1250\n",
            "Epoch 103/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1719 - accuracy: 0.6345 - val_loss: 2.7870 - val_accuracy: 0.1250\n",
            "Epoch 104/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1715 - accuracy: 0.6348 - val_loss: 2.7886 - val_accuracy: 0.1250\n",
            "Epoch 105/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1754 - accuracy: 0.6296 - val_loss: 2.8191 - val_accuracy: 0.1250\n",
            "Epoch 106/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1750 - accuracy: 0.6310 - val_loss: 2.8256 - val_accuracy: 0.1250\n",
            "Epoch 107/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1755 - accuracy: 0.6326 - val_loss: 2.8777 - val_accuracy: 0.1250\n",
            "Epoch 108/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1591 - accuracy: 0.6378 - val_loss: 2.8543 - val_accuracy: 0.1250\n",
            "Epoch 109/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1732 - accuracy: 0.6355 - val_loss: 2.8154 - val_accuracy: 0.1250\n",
            "Epoch 110/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1607 - accuracy: 0.6350 - val_loss: 2.8135 - val_accuracy: 0.1250\n",
            "Epoch 111/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1598 - accuracy: 0.6366 - val_loss: 2.9592 - val_accuracy: 0.1250\n",
            "Epoch 112/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1615 - accuracy: 0.6353 - val_loss: 2.8530 - val_accuracy: 0.1250\n",
            "Epoch 113/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1554 - accuracy: 0.6350 - val_loss: 2.9377 - val_accuracy: 0.1250\n",
            "Epoch 114/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1584 - accuracy: 0.6378 - val_loss: 2.8648 - val_accuracy: 0.1250\n",
            "Epoch 115/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1466 - accuracy: 0.6390 - val_loss: 2.9967 - val_accuracy: 0.1250\n",
            "Epoch 116/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1468 - accuracy: 0.6362 - val_loss: 3.0426 - val_accuracy: 0.1250\n",
            "Epoch 117/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1609 - accuracy: 0.6390 - val_loss: 2.9630 - val_accuracy: 0.1250\n",
            "Epoch 118/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1587 - accuracy: 0.6340 - val_loss: 3.0699 - val_accuracy: 0.1250\n",
            "Epoch 119/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1436 - accuracy: 0.6439 - val_loss: 2.9554 - val_accuracy: 0.1250\n",
            "Epoch 120/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1649 - accuracy: 0.6316 - val_loss: 3.1033 - val_accuracy: 0.1250\n",
            "Epoch 121/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1537 - accuracy: 0.6400 - val_loss: 2.9277 - val_accuracy: 0.1250\n",
            "Epoch 122/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1534 - accuracy: 0.6369 - val_loss: 3.1656 - val_accuracy: 0.1250\n",
            "Epoch 123/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1507 - accuracy: 0.6383 - val_loss: 2.9719 - val_accuracy: 0.1250\n",
            "Epoch 124/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1365 - accuracy: 0.6422 - val_loss: 3.0862 - val_accuracy: 0.1250\n",
            "Epoch 125/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1506 - accuracy: 0.6423 - val_loss: 3.0883 - val_accuracy: 0.1250\n",
            "Epoch 126/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1359 - accuracy: 0.6391 - val_loss: 3.2187 - val_accuracy: 0.1250\n",
            "Epoch 127/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1402 - accuracy: 0.6417 - val_loss: 3.1379 - val_accuracy: 0.1250\n",
            "Epoch 128/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1308 - accuracy: 0.6440 - val_loss: 3.0528 - val_accuracy: 0.1250\n",
            "Epoch 129/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1473 - accuracy: 0.6376 - val_loss: 3.1233 - val_accuracy: 0.1250\n",
            "Epoch 130/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1381 - accuracy: 0.6398 - val_loss: 3.1229 - val_accuracy: 0.1250\n",
            "Epoch 131/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1407 - accuracy: 0.6407 - val_loss: 3.1889 - val_accuracy: 0.1250\n",
            "Epoch 132/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1398 - accuracy: 0.6423 - val_loss: 3.2318 - val_accuracy: 0.1250\n",
            "Epoch 133/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1252 - accuracy: 0.6422 - val_loss: 3.1183 - val_accuracy: 0.1250\n",
            "Epoch 134/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1305 - accuracy: 0.6480 - val_loss: 3.2521 - val_accuracy: 0.1250\n",
            "Epoch 135/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1484 - accuracy: 0.6375 - val_loss: 3.2598 - val_accuracy: 0.1250\n",
            "Epoch 136/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1152 - accuracy: 0.6498 - val_loss: 3.2192 - val_accuracy: 0.1250\n",
            "Epoch 137/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1456 - accuracy: 0.6385 - val_loss: 3.3789 - val_accuracy: 0.1250\n",
            "Epoch 138/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1306 - accuracy: 0.6437 - val_loss: 3.2726 - val_accuracy: 0.1250\n",
            "Epoch 139/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1268 - accuracy: 0.6440 - val_loss: 3.3797 - val_accuracy: 0.1250\n",
            "Epoch 140/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1256 - accuracy: 0.6471 - val_loss: 3.4067 - val_accuracy: 0.1250\n",
            "Epoch 141/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1169 - accuracy: 0.6511 - val_loss: 3.3840 - val_accuracy: 0.1250\n",
            "Epoch 142/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1064 - accuracy: 0.6474 - val_loss: 3.4607 - val_accuracy: 0.1250\n",
            "Epoch 143/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1125 - accuracy: 0.6464 - val_loss: 3.5001 - val_accuracy: 0.1250\n",
            "Epoch 144/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1389 - accuracy: 0.6445 - val_loss: 3.5054 - val_accuracy: 0.1250\n",
            "Epoch 145/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.1080 - accuracy: 0.6459 - val_loss: 3.3318 - val_accuracy: 0.1250\n",
            "Epoch 146/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1344 - accuracy: 0.6416 - val_loss: 3.4495 - val_accuracy: 0.1250\n",
            "Epoch 147/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1186 - accuracy: 0.6461 - val_loss: 3.5199 - val_accuracy: 0.1250\n",
            "Epoch 148/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1156 - accuracy: 0.6494 - val_loss: 3.5086 - val_accuracy: 0.1250\n",
            "Epoch 149/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1116 - accuracy: 0.6505 - val_loss: 3.6456 - val_accuracy: 0.1250\n",
            "Epoch 150/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1059 - accuracy: 0.6493 - val_loss: 3.5800 - val_accuracy: 0.1250\n",
            "Epoch 151/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1076 - accuracy: 0.6477 - val_loss: 3.6948 - val_accuracy: 0.0000e+00\n",
            "Epoch 152/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1122 - accuracy: 0.6468 - val_loss: 3.6226 - val_accuracy: 0.1250\n",
            "Epoch 153/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1013 - accuracy: 0.6514 - val_loss: 3.7083 - val_accuracy: 0.1250\n",
            "Epoch 154/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1026 - accuracy: 0.6480 - val_loss: 3.7486 - val_accuracy: 0.0000e+00\n",
            "Epoch 155/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1018 - accuracy: 0.6482 - val_loss: 3.6452 - val_accuracy: 0.2500\n",
            "Epoch 156/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0972 - accuracy: 0.6517 - val_loss: 3.7752 - val_accuracy: 0.0000e+00\n",
            "Epoch 157/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1038 - accuracy: 0.6538 - val_loss: 3.7140 - val_accuracy: 0.0000e+00\n",
            "Epoch 158/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0962 - accuracy: 0.6511 - val_loss: 3.6900 - val_accuracy: 0.0000e+00\n",
            "Epoch 159/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1059 - accuracy: 0.6519 - val_loss: 3.8394 - val_accuracy: 0.0000e+00\n",
            "Epoch 160/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0957 - accuracy: 0.6532 - val_loss: 3.8310 - val_accuracy: 0.0000e+00\n",
            "Epoch 161/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1009 - accuracy: 0.6472 - val_loss: 3.7900 - val_accuracy: 0.0000e+00\n",
            "Epoch 162/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0916 - accuracy: 0.6530 - val_loss: 3.7749 - val_accuracy: 0.2500\n",
            "Epoch 163/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.1042 - accuracy: 0.6520 - val_loss: 3.8801 - val_accuracy: 0.0000e+00\n",
            "Epoch 164/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0849 - accuracy: 0.6525 - val_loss: 3.9382 - val_accuracy: 0.0000e+00\n",
            "Epoch 165/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0966 - accuracy: 0.6519 - val_loss: 3.9820 - val_accuracy: 0.0000e+00\n",
            "Epoch 166/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0942 - accuracy: 0.6520 - val_loss: 3.8984 - val_accuracy: 0.2500\n",
            "Epoch 167/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0990 - accuracy: 0.6516 - val_loss: 3.9129 - val_accuracy: 0.0000e+00\n",
            "Epoch 168/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0979 - accuracy: 0.6514 - val_loss: 3.8910 - val_accuracy: 0.1250\n",
            "Epoch 169/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1012 - accuracy: 0.6545 - val_loss: 3.9432 - val_accuracy: 0.1250\n",
            "Epoch 170/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0808 - accuracy: 0.6554 - val_loss: 3.9827 - val_accuracy: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0745 - accuracy: 0.6575 - val_loss: 4.0547 - val_accuracy: 0.0000e+00\n",
            "Epoch 172/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0854 - accuracy: 0.6543 - val_loss: 4.1149 - val_accuracy: 0.0000e+00\n",
            "Epoch 173/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0962 - accuracy: 0.6511 - val_loss: 4.0835 - val_accuracy: 0.1250\n",
            "Epoch 174/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0786 - accuracy: 0.6578 - val_loss: 4.0169 - val_accuracy: 0.1250\n",
            "Epoch 175/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0885 - accuracy: 0.6513 - val_loss: 4.0729 - val_accuracy: 0.0000e+00\n",
            "Epoch 176/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0875 - accuracy: 0.6556 - val_loss: 4.1836 - val_accuracy: 0.0000e+00\n",
            "Epoch 177/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0949 - accuracy: 0.6558 - val_loss: 4.2084 - val_accuracy: 0.0000e+00\n",
            "Epoch 178/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0797 - accuracy: 0.6598 - val_loss: 4.0890 - val_accuracy: 0.0000e+00\n",
            "Epoch 179/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0857 - accuracy: 0.6554 - val_loss: 4.1682 - val_accuracy: 0.0000e+00\n",
            "Epoch 180/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0784 - accuracy: 0.6538 - val_loss: 4.1874 - val_accuracy: 0.0000e+00\n",
            "Epoch 181/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0928 - accuracy: 0.6518 - val_loss: 4.2544 - val_accuracy: 0.0000e+00\n",
            "Epoch 182/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0743 - accuracy: 0.6549 - val_loss: 4.3706 - val_accuracy: 0.0000e+00\n",
            "Epoch 183/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0943 - accuracy: 0.6525 - val_loss: 4.2755 - val_accuracy: 0.0000e+00\n",
            "Epoch 184/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0766 - accuracy: 0.6578 - val_loss: 4.3398 - val_accuracy: 0.0000e+00\n",
            "Epoch 185/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0855 - accuracy: 0.6537 - val_loss: 4.2454 - val_accuracy: 0.2500\n",
            "Epoch 186/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0898 - accuracy: 0.6516 - val_loss: 4.1884 - val_accuracy: 0.2500\n",
            "Epoch 187/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0754 - accuracy: 0.6601 - val_loss: 4.2650 - val_accuracy: 0.0000e+00\n",
            "Epoch 188/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0802 - accuracy: 0.6573 - val_loss: 4.1084 - val_accuracy: 0.1250\n",
            "Epoch 189/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0840 - accuracy: 0.6561 - val_loss: 4.3497 - val_accuracy: 0.1250\n",
            "Epoch 190/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0822 - accuracy: 0.6577 - val_loss: 4.3770 - val_accuracy: 0.0000e+00\n",
            "Epoch 191/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0856 - accuracy: 0.6529 - val_loss: 4.4222 - val_accuracy: 0.2500\n",
            "Epoch 192/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0784 - accuracy: 0.6554 - val_loss: 4.2622 - val_accuracy: 0.2500\n",
            "Epoch 193/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0762 - accuracy: 0.6597 - val_loss: 4.3914 - val_accuracy: 0.2500\n",
            "Epoch 194/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0805 - accuracy: 0.6573 - val_loss: 4.3274 - val_accuracy: 0.2500\n",
            "Epoch 195/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0671 - accuracy: 0.6601 - val_loss: 4.4493 - val_accuracy: 0.0000e+00\n",
            "Epoch 196/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0718 - accuracy: 0.6583 - val_loss: 4.5712 - val_accuracy: 0.0000e+00\n",
            "Epoch 197/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0609 - accuracy: 0.6615 - val_loss: 4.6025 - val_accuracy: 0.0000e+00\n",
            "Epoch 198/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0768 - accuracy: 0.6617 - val_loss: 4.6153 - val_accuracy: 0.1250\n",
            "Epoch 199/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0822 - accuracy: 0.6552 - val_loss: 4.5322 - val_accuracy: 0.2500\n",
            "Epoch 200/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0721 - accuracy: 0.6556 - val_loss: 4.7277 - val_accuracy: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0711 - accuracy: 0.6623 - val_loss: 4.5027 - val_accuracy: 0.2500\n",
            "Epoch 202/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0737 - accuracy: 0.6524 - val_loss: 4.6821 - val_accuracy: 0.0000e+00\n",
            "Epoch 203/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0687 - accuracy: 0.6567 - val_loss: 4.4748 - val_accuracy: 0.2500\n",
            "Epoch 204/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0732 - accuracy: 0.6547 - val_loss: 4.3886 - val_accuracy: 0.2500\n",
            "Epoch 205/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0723 - accuracy: 0.6566 - val_loss: 4.7497 - val_accuracy: 0.2500\n",
            "Epoch 206/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0741 - accuracy: 0.6586 - val_loss: 4.5783 - val_accuracy: 0.2500\n",
            "Epoch 207/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0705 - accuracy: 0.6571 - val_loss: 4.6426 - val_accuracy: 0.2500\n",
            "Epoch 208/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0729 - accuracy: 0.6615 - val_loss: 4.6514 - val_accuracy: 0.2500\n",
            "Epoch 209/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0619 - accuracy: 0.6600 - val_loss: 4.7323 - val_accuracy: 0.1250\n",
            "Epoch 210/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0653 - accuracy: 0.6601 - val_loss: 4.8147 - val_accuracy: 0.2500\n",
            "Epoch 211/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0600 - accuracy: 0.6636 - val_loss: 4.7416 - val_accuracy: 0.1250\n",
            "Epoch 212/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0682 - accuracy: 0.6608 - val_loss: 4.6753 - val_accuracy: 0.2500\n",
            "Epoch 213/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0565 - accuracy: 0.6608 - val_loss: 4.5508 - val_accuracy: 0.2500\n",
            "Epoch 214/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0631 - accuracy: 0.6635 - val_loss: 4.8177 - val_accuracy: 0.2500\n",
            "Epoch 215/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0706 - accuracy: 0.6572 - val_loss: 4.7074 - val_accuracy: 0.2500\n",
            "Epoch 216/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0625 - accuracy: 0.6617 - val_loss: 4.5548 - val_accuracy: 0.1250\n",
            "Epoch 217/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0579 - accuracy: 0.6634 - val_loss: 4.6964 - val_accuracy: 0.2500\n",
            "Epoch 218/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0437 - accuracy: 0.6628 - val_loss: 4.8435 - val_accuracy: 0.1250\n",
            "Epoch 219/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0559 - accuracy: 0.6624 - val_loss: 5.0904 - val_accuracy: 0.2500\n",
            "Epoch 220/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0598 - accuracy: 0.6627 - val_loss: 4.8713 - val_accuracy: 0.2500\n",
            "Epoch 221/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0599 - accuracy: 0.6588 - val_loss: 4.9699 - val_accuracy: 0.2500\n",
            "Epoch 222/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0616 - accuracy: 0.6622 - val_loss: 4.9170 - val_accuracy: 0.2500\n",
            "Epoch 223/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0593 - accuracy: 0.6652 - val_loss: 4.7067 - val_accuracy: 0.2500\n",
            "Epoch 224/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0610 - accuracy: 0.6627 - val_loss: 5.0655 - val_accuracy: 0.0000e+00\n",
            "Epoch 225/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0575 - accuracy: 0.6615 - val_loss: 5.0195 - val_accuracy: 0.2500\n",
            "Epoch 226/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0605 - accuracy: 0.6598 - val_loss: 5.1556 - val_accuracy: 0.1250\n",
            "Epoch 227/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0636 - accuracy: 0.6592 - val_loss: 5.0113 - val_accuracy: 0.2500\n",
            "Epoch 228/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0423 - accuracy: 0.6714 - val_loss: 4.8721 - val_accuracy: 0.2500\n",
            "Epoch 229/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0431 - accuracy: 0.6674 - val_loss: 5.1961 - val_accuracy: 0.2500\n",
            "Epoch 230/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0568 - accuracy: 0.6600 - val_loss: 5.2526 - val_accuracy: 0.2500\n",
            "Epoch 231/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0451 - accuracy: 0.6671 - val_loss: 5.1503 - val_accuracy: 0.2500\n",
            "Epoch 232/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0420 - accuracy: 0.6633 - val_loss: 5.1922 - val_accuracy: 0.1250\n",
            "Epoch 233/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0396 - accuracy: 0.6666 - val_loss: 5.0968 - val_accuracy: 0.2500\n",
            "Epoch 234/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0601 - accuracy: 0.6621 - val_loss: 5.1796 - val_accuracy: 0.1250\n",
            "Epoch 235/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0496 - accuracy: 0.6628 - val_loss: 5.2571 - val_accuracy: 0.1250\n",
            "Epoch 236/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0536 - accuracy: 0.6654 - val_loss: 5.3222 - val_accuracy: 0.1250\n",
            "Epoch 237/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0622 - accuracy: 0.6602 - val_loss: 5.0590 - val_accuracy: 0.2500\n",
            "Epoch 238/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0367 - accuracy: 0.6669 - val_loss: 5.2667 - val_accuracy: 0.2500\n",
            "Epoch 239/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0522 - accuracy: 0.6642 - val_loss: 5.0959 - val_accuracy: 0.0000e+00\n",
            "Epoch 240/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0517 - accuracy: 0.6657 - val_loss: 5.2875 - val_accuracy: 0.0000e+00\n",
            "Epoch 241/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0437 - accuracy: 0.6660 - val_loss: 5.1536 - val_accuracy: 0.0000e+00\n",
            "Epoch 242/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0514 - accuracy: 0.6639 - val_loss: 5.3498 - val_accuracy: 0.1250\n",
            "Epoch 243/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0551 - accuracy: 0.6645 - val_loss: 5.3100 - val_accuracy: 0.2500\n",
            "Epoch 244/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0401 - accuracy: 0.6705 - val_loss: 5.5394 - val_accuracy: 0.0000e+00\n",
            "Epoch 245/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0428 - accuracy: 0.6648 - val_loss: 5.3192 - val_accuracy: 0.2500\n",
            "Epoch 246/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0558 - accuracy: 0.6610 - val_loss: 5.3047 - val_accuracy: 0.1250\n",
            "Epoch 247/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0516 - accuracy: 0.6655 - val_loss: 5.4769 - val_accuracy: 0.0000e+00\n",
            "Epoch 248/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0428 - accuracy: 0.6722 - val_loss: 5.3550 - val_accuracy: 0.1250\n",
            "Epoch 249/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0364 - accuracy: 0.6679 - val_loss: 5.4783 - val_accuracy: 0.0000e+00\n",
            "Epoch 250/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0526 - accuracy: 0.6685 - val_loss: 5.2097 - val_accuracy: 0.0000e+00\n",
            "Epoch 251/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0469 - accuracy: 0.6654 - val_loss: 5.3515 - val_accuracy: 0.0000e+00\n",
            "Epoch 252/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0510 - accuracy: 0.6652 - val_loss: 5.3460 - val_accuracy: 0.1250\n",
            "Epoch 253/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0505 - accuracy: 0.6629 - val_loss: 5.6159 - val_accuracy: 0.0000e+00\n",
            "Epoch 254/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0649 - accuracy: 0.6631 - val_loss: 5.4726 - val_accuracy: 0.0000e+00\n",
            "Epoch 255/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0339 - accuracy: 0.6699 - val_loss: 5.3010 - val_accuracy: 0.0000e+00\n",
            "Epoch 256/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0492 - accuracy: 0.6638 - val_loss: 5.3894 - val_accuracy: 0.1250\n",
            "Epoch 257/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0459 - accuracy: 0.6650 - val_loss: 5.4982 - val_accuracy: 0.0000e+00\n",
            "Epoch 258/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0474 - accuracy: 0.6618 - val_loss: 5.5986 - val_accuracy: 0.0000e+00\n",
            "Epoch 259/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0408 - accuracy: 0.6658 - val_loss: 5.5996 - val_accuracy: 0.0000e+00\n",
            "Epoch 260/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0510 - accuracy: 0.6607 - val_loss: 5.6326 - val_accuracy: 0.0000e+00\n",
            "Epoch 261/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0376 - accuracy: 0.6697 - val_loss: 5.5158 - val_accuracy: 0.0000e+00\n",
            "Epoch 262/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0390 - accuracy: 0.6679 - val_loss: 5.5531 - val_accuracy: 0.1250\n",
            "Epoch 263/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0530 - accuracy: 0.6614 - val_loss: 5.4561 - val_accuracy: 0.0000e+00\n",
            "Epoch 264/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0387 - accuracy: 0.6693 - val_loss: 5.3358 - val_accuracy: 0.0000e+00\n",
            "Epoch 265/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0440 - accuracy: 0.6689 - val_loss: 5.7344 - val_accuracy: 0.2500\n",
            "Epoch 266/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0412 - accuracy: 0.6650 - val_loss: 5.7061 - val_accuracy: 0.0000e+00\n",
            "Epoch 267/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0374 - accuracy: 0.6728 - val_loss: 5.8765 - val_accuracy: 0.1250\n",
            "Epoch 268/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0438 - accuracy: 0.6676 - val_loss: 5.7774 - val_accuracy: 0.0000e+00\n",
            "Epoch 269/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0185 - accuracy: 0.6727 - val_loss: 5.4249 - val_accuracy: 0.0000e+00\n",
            "Epoch 270/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0249 - accuracy: 0.6683 - val_loss: 5.8324 - val_accuracy: 0.1250\n",
            "Epoch 271/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0263 - accuracy: 0.6705 - val_loss: 5.7638 - val_accuracy: 0.0000e+00\n",
            "Epoch 272/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0358 - accuracy: 0.6719 - val_loss: 5.6773 - val_accuracy: 0.0000e+00\n",
            "Epoch 273/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0227 - accuracy: 0.6735 - val_loss: 5.6471 - val_accuracy: 0.0000e+00\n",
            "Epoch 274/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0285 - accuracy: 0.6736 - val_loss: 5.4518 - val_accuracy: 0.0000e+00\n",
            "Epoch 275/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0296 - accuracy: 0.6704 - val_loss: 5.9341 - val_accuracy: 0.0000e+00\n",
            "Epoch 276/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0377 - accuracy: 0.6709 - val_loss: 5.4442 - val_accuracy: 0.1250\n",
            "Epoch 277/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0286 - accuracy: 0.6728 - val_loss: 5.8908 - val_accuracy: 0.0000e+00\n",
            "Epoch 278/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0323 - accuracy: 0.6716 - val_loss: 5.8379 - val_accuracy: 0.0000e+00\n",
            "Epoch 279/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0210 - accuracy: 0.6754 - val_loss: 5.9561 - val_accuracy: 0.0000e+00\n",
            "Epoch 280/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0314 - accuracy: 0.6695 - val_loss: 5.7767 - val_accuracy: 0.0000e+00\n",
            "Epoch 281/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0361 - accuracy: 0.6716 - val_loss: 5.9216 - val_accuracy: 0.0000e+00\n",
            "Epoch 282/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0361 - accuracy: 0.6743 - val_loss: 5.8350 - val_accuracy: 0.0000e+00\n",
            "Epoch 283/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0328 - accuracy: 0.6711 - val_loss: 5.8849 - val_accuracy: 0.0000e+00\n",
            "Epoch 284/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0344 - accuracy: 0.6706 - val_loss: 5.5019 - val_accuracy: 0.1250\n",
            "Epoch 285/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0342 - accuracy: 0.6687 - val_loss: 6.0361 - val_accuracy: 0.0000e+00\n",
            "Epoch 286/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0249 - accuracy: 0.6720 - val_loss: 5.8008 - val_accuracy: 0.1250\n",
            "Epoch 287/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0265 - accuracy: 0.6753 - val_loss: 6.2107 - val_accuracy: 0.0000e+00\n",
            "Epoch 288/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0155 - accuracy: 0.6729 - val_loss: 6.0394 - val_accuracy: 0.0000e+00\n",
            "Epoch 289/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0238 - accuracy: 0.6719 - val_loss: 6.3324 - val_accuracy: 0.0000e+00\n",
            "Epoch 290/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0284 - accuracy: 0.6726 - val_loss: 6.1581 - val_accuracy: 0.0000e+00\n",
            "Epoch 291/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0437 - accuracy: 0.6692 - val_loss: 5.9437 - val_accuracy: 0.0000e+00\n",
            "Epoch 292/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0177 - accuracy: 0.6756 - val_loss: 6.0408 - val_accuracy: 0.0000e+00\n",
            "Epoch 293/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0272 - accuracy: 0.6683 - val_loss: 6.1157 - val_accuracy: 0.1250\n",
            "Epoch 294/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0213 - accuracy: 0.6763 - val_loss: 6.2495 - val_accuracy: 0.0000e+00\n",
            "Epoch 295/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0302 - accuracy: 0.6688 - val_loss: 6.0075 - val_accuracy: 0.0000e+00\n",
            "Epoch 296/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0239 - accuracy: 0.6707 - val_loss: 6.1609 - val_accuracy: 0.1250\n",
            "Epoch 297/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0309 - accuracy: 0.6723 - val_loss: 6.2577 - val_accuracy: 0.1250\n",
            "Epoch 298/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0160 - accuracy: 0.6754 - val_loss: 6.0944 - val_accuracy: 0.1250\n",
            "Epoch 299/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0290 - accuracy: 0.6711 - val_loss: 6.2758 - val_accuracy: 0.1250\n",
            "Epoch 300/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0158 - accuracy: 0.6756 - val_loss: 6.1185 - val_accuracy: 0.0000e+00\n",
            "Epoch 301/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0281 - accuracy: 0.6772 - val_loss: 6.1185 - val_accuracy: 0.0000e+00\n",
            "Epoch 302/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0312 - accuracy: 0.6699 - val_loss: 5.8522 - val_accuracy: 0.1250\n",
            "Epoch 303/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0299 - accuracy: 0.6728 - val_loss: 6.1160 - val_accuracy: 0.1250\n",
            "Epoch 304/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0315 - accuracy: 0.6710 - val_loss: 6.1094 - val_accuracy: 0.0000e+00\n",
            "Epoch 305/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0356 - accuracy: 0.6683 - val_loss: 6.1237 - val_accuracy: 0.0000e+00\n",
            "Epoch 306/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9964 - accuracy: 0.6770 - val_loss: 5.8703 - val_accuracy: 0.1250\n",
            "Epoch 307/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0411 - accuracy: 0.6685 - val_loss: 6.1499 - val_accuracy: 0.1250\n",
            "Epoch 308/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0246 - accuracy: 0.6749 - val_loss: 6.1679 - val_accuracy: 0.1250\n",
            "Epoch 309/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0205 - accuracy: 0.6765 - val_loss: 5.9628 - val_accuracy: 0.1250\n",
            "Epoch 310/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0146 - accuracy: 0.6752 - val_loss: 6.2533 - val_accuracy: 0.1250\n",
            "Epoch 311/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0219 - accuracy: 0.6744 - val_loss: 5.9580 - val_accuracy: 0.1250\n",
            "Epoch 312/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0128 - accuracy: 0.6756 - val_loss: 6.2913 - val_accuracy: 0.1250\n",
            "Epoch 313/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0264 - accuracy: 0.6742 - val_loss: 6.2494 - val_accuracy: 0.1250\n",
            "Epoch 314/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0009 - accuracy: 0.6791 - val_loss: 6.2680 - val_accuracy: 0.1250\n",
            "Epoch 315/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0137 - accuracy: 0.6764 - val_loss: 6.2945 - val_accuracy: 0.1250\n",
            "Epoch 316/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0036 - accuracy: 0.6776 - val_loss: 6.4541 - val_accuracy: 0.1250\n",
            "Epoch 317/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0117 - accuracy: 0.6753 - val_loss: 6.4420 - val_accuracy: 0.1250\n",
            "Epoch 318/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0165 - accuracy: 0.6735 - val_loss: 6.3487 - val_accuracy: 0.1250\n",
            "Epoch 319/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0152 - accuracy: 0.6754 - val_loss: 6.3742 - val_accuracy: 0.1250\n",
            "Epoch 320/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0162 - accuracy: 0.6778 - val_loss: 6.1629 - val_accuracy: 0.1250\n",
            "Epoch 321/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0276 - accuracy: 0.6772 - val_loss: 6.5847 - val_accuracy: 0.0000e+00\n",
            "Epoch 322/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0092 - accuracy: 0.6799 - val_loss: 6.7631 - val_accuracy: 0.1250\n",
            "Epoch 323/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0076 - accuracy: 0.6774 - val_loss: 6.6641 - val_accuracy: 0.1250\n",
            "Epoch 324/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0214 - accuracy: 0.6761 - val_loss: 6.3571 - val_accuracy: 0.1250\n",
            "Epoch 325/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9973 - accuracy: 0.6789 - val_loss: 6.5013 - val_accuracy: 0.1250\n",
            "Epoch 326/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0112 - accuracy: 0.6748 - val_loss: 6.3658 - val_accuracy: 0.1250\n",
            "Epoch 327/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0125 - accuracy: 0.6786 - val_loss: 6.3687 - val_accuracy: 0.1250\n",
            "Epoch 328/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0143 - accuracy: 0.6716 - val_loss: 6.4553 - val_accuracy: 0.1250\n",
            "Epoch 329/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0007 - accuracy: 0.6836 - val_loss: 6.3508 - val_accuracy: 0.1250\n",
            "Epoch 330/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9944 - accuracy: 0.6857 - val_loss: 6.4446 - val_accuracy: 0.1250\n",
            "Epoch 331/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0009 - accuracy: 0.6803 - val_loss: 6.3436 - val_accuracy: 0.1250\n",
            "Epoch 332/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0163 - accuracy: 0.6752 - val_loss: 7.0475 - val_accuracy: 0.0000e+00\n",
            "Epoch 333/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9951 - accuracy: 0.6827 - val_loss: 6.4997 - val_accuracy: 0.1250\n",
            "Epoch 334/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0083 - accuracy: 0.6753 - val_loss: 6.6432 - val_accuracy: 0.1250\n",
            "Epoch 335/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9951 - accuracy: 0.6775 - val_loss: 6.7693 - val_accuracy: 0.1250\n",
            "Epoch 336/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0000 - accuracy: 0.6799 - val_loss: 6.7143 - val_accuracy: 0.1250\n",
            "Epoch 337/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0049 - accuracy: 0.6784 - val_loss: 6.1620 - val_accuracy: 0.1250\n",
            "Epoch 338/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0056 - accuracy: 0.6820 - val_loss: 6.6658 - val_accuracy: 0.1250\n",
            "Epoch 339/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0130 - accuracy: 0.6751 - val_loss: 6.6056 - val_accuracy: 0.1250\n",
            "Epoch 340/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9899 - accuracy: 0.6831 - val_loss: 6.5121 - val_accuracy: 0.1250\n",
            "Epoch 341/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9969 - accuracy: 0.6829 - val_loss: 6.7442 - val_accuracy: 0.1250\n",
            "Epoch 342/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9919 - accuracy: 0.6840 - val_loss: 6.8731 - val_accuracy: 0.1250\n",
            "Epoch 343/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0076 - accuracy: 0.6785 - val_loss: 6.9356 - val_accuracy: 0.1250\n",
            "Epoch 344/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9963 - accuracy: 0.6812 - val_loss: 6.7064 - val_accuracy: 0.1250\n",
            "Epoch 345/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0000 - accuracy: 0.6827 - val_loss: 6.5914 - val_accuracy: 0.1250\n",
            "Epoch 346/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 1.0011 - accuracy: 0.6816 - val_loss: 6.4808 - val_accuracy: 0.1250\n",
            "Epoch 347/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9963 - accuracy: 0.6804 - val_loss: 6.8109 - val_accuracy: 0.1250\n",
            "Epoch 348/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9979 - accuracy: 0.6822 - val_loss: 7.0867 - val_accuracy: 0.1250\n",
            "Epoch 349/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.0045 - accuracy: 0.6822 - val_loss: 6.8950 - val_accuracy: 0.1250\n",
            "Epoch 350/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9843 - accuracy: 0.6848 - val_loss: 6.7365 - val_accuracy: 0.1250\n",
            "Epoch 351/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0011 - accuracy: 0.6820 - val_loss: 6.9236 - val_accuracy: 0.1250\n",
            "Epoch 352/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9956 - accuracy: 0.6840 - val_loss: 7.0743 - val_accuracy: 0.1250\n",
            "Epoch 353/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9994 - accuracy: 0.6768 - val_loss: 6.9783 - val_accuracy: 0.1250\n",
            "Epoch 354/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9855 - accuracy: 0.6828 - val_loss: 6.9766 - val_accuracy: 0.1250\n",
            "Epoch 355/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0007 - accuracy: 0.6785 - val_loss: 7.1255 - val_accuracy: 0.1250\n",
            "Epoch 356/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9930 - accuracy: 0.6824 - val_loss: 6.4833 - val_accuracy: 0.1250\n",
            "Epoch 357/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0029 - accuracy: 0.6811 - val_loss: 6.9231 - val_accuracy: 0.1250\n",
            "Epoch 358/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9949 - accuracy: 0.6836 - val_loss: 6.9279 - val_accuracy: 0.1250\n",
            "Epoch 359/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9942 - accuracy: 0.6843 - val_loss: 7.0023 - val_accuracy: 0.1250\n",
            "Epoch 360/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9950 - accuracy: 0.6806 - val_loss: 6.8650 - val_accuracy: 0.1250\n",
            "Epoch 361/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9944 - accuracy: 0.6829 - val_loss: 7.0015 - val_accuracy: 0.1250\n",
            "Epoch 362/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9824 - accuracy: 0.6831 - val_loss: 6.8894 - val_accuracy: 0.1250\n",
            "Epoch 363/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9925 - accuracy: 0.6841 - val_loss: 7.0289 - val_accuracy: 0.1250\n",
            "Epoch 364/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9823 - accuracy: 0.6892 - val_loss: 6.7200 - val_accuracy: 0.1250\n",
            "Epoch 365/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9902 - accuracy: 0.6855 - val_loss: 7.1595 - val_accuracy: 0.1250\n",
            "Epoch 366/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9892 - accuracy: 0.6808 - val_loss: 7.1765 - val_accuracy: 0.1250\n",
            "Epoch 367/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9937 - accuracy: 0.6844 - val_loss: 7.2967 - val_accuracy: 0.1250\n",
            "Epoch 368/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9822 - accuracy: 0.6860 - val_loss: 6.8694 - val_accuracy: 0.1250\n",
            "Epoch 369/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9784 - accuracy: 0.6867 - val_loss: 7.1551 - val_accuracy: 0.1250\n",
            "Epoch 370/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9744 - accuracy: 0.6880 - val_loss: 6.8086 - val_accuracy: 0.1250\n",
            "Epoch 371/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9897 - accuracy: 0.6845 - val_loss: 7.0658 - val_accuracy: 0.1250\n",
            "Epoch 372/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9909 - accuracy: 0.6849 - val_loss: 6.9574 - val_accuracy: 0.1250\n",
            "Epoch 373/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9770 - accuracy: 0.6843 - val_loss: 7.0603 - val_accuracy: 0.1250\n",
            "Epoch 374/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.6834 - val_loss: 7.2137 - val_accuracy: 0.1250\n",
            "Epoch 375/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9705 - accuracy: 0.6889 - val_loss: 7.0742 - val_accuracy: 0.1250\n",
            "Epoch 376/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9773 - accuracy: 0.6863 - val_loss: 6.8980 - val_accuracy: 0.1250\n",
            "Epoch 377/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9798 - accuracy: 0.6863 - val_loss: 6.8983 - val_accuracy: 0.1250\n",
            "Epoch 378/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9681 - accuracy: 0.6904 - val_loss: 7.1960 - val_accuracy: 0.1250\n",
            "Epoch 379/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9834 - accuracy: 0.6878 - val_loss: 7.1689 - val_accuracy: 0.1250\n",
            "Epoch 380/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9779 - accuracy: 0.6870 - val_loss: 6.8955 - val_accuracy: 0.1250\n",
            "Epoch 381/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9693 - accuracy: 0.6890 - val_loss: 7.0808 - val_accuracy: 0.1250\n",
            "Epoch 382/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9732 - accuracy: 0.6910 - val_loss: 7.2489 - val_accuracy: 0.1250\n",
            "Epoch 383/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9754 - accuracy: 0.6876 - val_loss: 7.0386 - val_accuracy: 0.1250\n",
            "Epoch 384/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9816 - accuracy: 0.6902 - val_loss: 7.3462 - val_accuracy: 0.1250\n",
            "Epoch 385/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9810 - accuracy: 0.6856 - val_loss: 7.1375 - val_accuracy: 0.1250\n",
            "Epoch 386/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9817 - accuracy: 0.6868 - val_loss: 7.1142 - val_accuracy: 0.1250\n",
            "Epoch 387/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9829 - accuracy: 0.6846 - val_loss: 7.2603 - val_accuracy: 0.1250\n",
            "Epoch 388/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9807 - accuracy: 0.6899 - val_loss: 7.0232 - val_accuracy: 0.1250\n",
            "Epoch 389/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9750 - accuracy: 0.6863 - val_loss: 7.0863 - val_accuracy: 0.1250\n",
            "Epoch 390/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9763 - accuracy: 0.6881 - val_loss: 7.1862 - val_accuracy: 0.1250\n",
            "Epoch 391/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9767 - accuracy: 0.6857 - val_loss: 7.0826 - val_accuracy: 0.1250\n",
            "Epoch 392/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9734 - accuracy: 0.6932 - val_loss: 7.3693 - val_accuracy: 0.1250\n",
            "Epoch 393/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9901 - accuracy: 0.6860 - val_loss: 7.5450 - val_accuracy: 0.1250\n",
            "Epoch 394/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9787 - accuracy: 0.6890 - val_loss: 7.4069 - val_accuracy: 0.1250\n",
            "Epoch 395/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9629 - accuracy: 0.6911 - val_loss: 7.2031 - val_accuracy: 0.1250\n",
            "Epoch 396/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9808 - accuracy: 0.6921 - val_loss: 7.2956 - val_accuracy: 0.1250\n",
            "Epoch 397/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9840 - accuracy: 0.6840 - val_loss: 7.6284 - val_accuracy: 0.1250\n",
            "Epoch 398/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9719 - accuracy: 0.6905 - val_loss: 7.6611 - val_accuracy: 0.1250\n",
            "Epoch 399/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9748 - accuracy: 0.6901 - val_loss: 7.3099 - val_accuracy: 0.1250\n",
            "Epoch 400/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9893 - accuracy: 0.6877 - val_loss: 7.4702 - val_accuracy: 0.1250\n",
            "Epoch 401/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9672 - accuracy: 0.6912 - val_loss: 7.0505 - val_accuracy: 0.1250\n",
            "Epoch 402/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9683 - accuracy: 0.6906 - val_loss: 7.2962 - val_accuracy: 0.1250\n",
            "Epoch 403/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9746 - accuracy: 0.6917 - val_loss: 6.9192 - val_accuracy: 0.1250\n",
            "Epoch 404/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9689 - accuracy: 0.6958 - val_loss: 6.9915 - val_accuracy: 0.1250\n",
            "Epoch 405/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9748 - accuracy: 0.6905 - val_loss: 7.7050 - val_accuracy: 0.1250\n",
            "Epoch 406/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9700 - accuracy: 0.6945 - val_loss: 7.2077 - val_accuracy: 0.1250\n",
            "Epoch 407/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9755 - accuracy: 0.6847 - val_loss: 7.1671 - val_accuracy: 0.1250\n",
            "Epoch 408/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9694 - accuracy: 0.6921 - val_loss: 7.4837 - val_accuracy: 0.1250\n",
            "Epoch 409/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9649 - accuracy: 0.6890 - val_loss: 7.4990 - val_accuracy: 0.1250\n",
            "Epoch 410/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9543 - accuracy: 0.6957 - val_loss: 7.5940 - val_accuracy: 0.1250\n",
            "Epoch 411/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9679 - accuracy: 0.6867 - val_loss: 7.3674 - val_accuracy: 0.1250\n",
            "Epoch 412/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9656 - accuracy: 0.6902 - val_loss: 7.0937 - val_accuracy: 0.1250\n",
            "Epoch 413/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9623 - accuracy: 0.6918 - val_loss: 7.0734 - val_accuracy: 0.1250\n",
            "Epoch 414/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9667 - accuracy: 0.6929 - val_loss: 7.5031 - val_accuracy: 0.1250\n",
            "Epoch 415/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9587 - accuracy: 0.6950 - val_loss: 7.1472 - val_accuracy: 0.1250\n",
            "Epoch 416/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9601 - accuracy: 0.6928 - val_loss: 7.3387 - val_accuracy: 0.1250\n",
            "Epoch 417/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9616 - accuracy: 0.6947 - val_loss: 7.2720 - val_accuracy: 0.1250\n",
            "Epoch 418/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9676 - accuracy: 0.6922 - val_loss: 7.5569 - val_accuracy: 0.1250\n",
            "Epoch 419/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9670 - accuracy: 0.6933 - val_loss: 7.2174 - val_accuracy: 0.1250\n",
            "Epoch 420/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9738 - accuracy: 0.6869 - val_loss: 7.5432 - val_accuracy: 0.1250\n",
            "Epoch 421/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9681 - accuracy: 0.6910 - val_loss: 7.4174 - val_accuracy: 0.1250\n",
            "Epoch 422/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9608 - accuracy: 0.6947 - val_loss: 7.1054 - val_accuracy: 0.1250\n",
            "Epoch 423/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9721 - accuracy: 0.6900 - val_loss: 7.6503 - val_accuracy: 0.1250\n",
            "Epoch 424/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9570 - accuracy: 0.6939 - val_loss: 7.5316 - val_accuracy: 0.1250\n",
            "Epoch 425/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9523 - accuracy: 0.6939 - val_loss: 7.4375 - val_accuracy: 0.1250\n",
            "Epoch 426/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9549 - accuracy: 0.6947 - val_loss: 7.4090 - val_accuracy: 0.1250\n",
            "Epoch 427/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9648 - accuracy: 0.6907 - val_loss: 7.5391 - val_accuracy: 0.1250\n",
            "Epoch 428/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9611 - accuracy: 0.6933 - val_loss: 7.6718 - val_accuracy: 0.1250\n",
            "Epoch 429/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9646 - accuracy: 0.6934 - val_loss: 7.3587 - val_accuracy: 0.1250\n",
            "Epoch 430/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9504 - accuracy: 0.6923 - val_loss: 7.2739 - val_accuracy: 0.1250\n",
            "Epoch 431/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9592 - accuracy: 0.6933 - val_loss: 7.5983 - val_accuracy: 0.1250\n",
            "Epoch 432/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9831 - accuracy: 0.6886 - val_loss: 7.3605 - val_accuracy: 0.1250\n",
            "Epoch 433/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9624 - accuracy: 0.6909 - val_loss: 7.4665 - val_accuracy: 0.1250\n",
            "Epoch 434/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9609 - accuracy: 0.6933 - val_loss: 7.7638 - val_accuracy: 0.1250\n",
            "Epoch 435/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9544 - accuracy: 0.6974 - val_loss: 7.6564 - val_accuracy: 0.1250\n",
            "Epoch 436/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9667 - accuracy: 0.6939 - val_loss: 7.3590 - val_accuracy: 0.1250\n",
            "Epoch 437/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9758 - accuracy: 0.6896 - val_loss: 7.2343 - val_accuracy: 0.1250\n",
            "Epoch 438/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9536 - accuracy: 0.6951 - val_loss: 7.4144 - val_accuracy: 0.1250\n",
            "Epoch 439/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9604 - accuracy: 0.6915 - val_loss: 7.2908 - val_accuracy: 0.1250\n",
            "Epoch 440/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9686 - accuracy: 0.6934 - val_loss: 7.3368 - val_accuracy: 0.1250\n",
            "Epoch 441/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9554 - accuracy: 0.6952 - val_loss: 7.1167 - val_accuracy: 0.1250\n",
            "Epoch 442/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9626 - accuracy: 0.6929 - val_loss: 7.6256 - val_accuracy: 0.1250\n",
            "Epoch 443/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9617 - accuracy: 0.6893 - val_loss: 7.4814 - val_accuracy: 0.1250\n",
            "Epoch 444/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9673 - accuracy: 0.6931 - val_loss: 7.7252 - val_accuracy: 0.1250\n",
            "Epoch 445/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9671 - accuracy: 0.6929 - val_loss: 7.4023 - val_accuracy: 0.1250\n",
            "Epoch 446/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9605 - accuracy: 0.6965 - val_loss: 7.3863 - val_accuracy: 0.1250\n",
            "Epoch 447/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9668 - accuracy: 0.6931 - val_loss: 7.8855 - val_accuracy: 0.1250\n",
            "Epoch 448/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9544 - accuracy: 0.6969 - val_loss: 7.8277 - val_accuracy: 0.1250\n",
            "Epoch 449/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9671 - accuracy: 0.6903 - val_loss: 7.6840 - val_accuracy: 0.1250\n",
            "Epoch 450/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9588 - accuracy: 0.6952 - val_loss: 7.5711 - val_accuracy: 0.1250\n",
            "Epoch 451/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9575 - accuracy: 0.6940 - val_loss: 7.2166 - val_accuracy: 0.1250\n",
            "Epoch 452/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9680 - accuracy: 0.6931 - val_loss: 7.6901 - val_accuracy: 0.1250\n",
            "Epoch 453/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9603 - accuracy: 0.6941 - val_loss: 7.4699 - val_accuracy: 0.1250\n",
            "Epoch 454/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9582 - accuracy: 0.6954 - val_loss: 7.5939 - val_accuracy: 0.1250\n",
            "Epoch 455/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9533 - accuracy: 0.6934 - val_loss: 7.4934 - val_accuracy: 0.1250\n",
            "Epoch 456/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9557 - accuracy: 0.6978 - val_loss: 7.4734 - val_accuracy: 0.1250\n",
            "Epoch 457/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9643 - accuracy: 0.6898 - val_loss: 7.9469 - val_accuracy: 0.1250\n",
            "Epoch 458/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9560 - accuracy: 0.6998 - val_loss: 7.6524 - val_accuracy: 0.1250\n",
            "Epoch 459/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9510 - accuracy: 0.6944 - val_loss: 7.3909 - val_accuracy: 0.1250\n",
            "Epoch 460/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9428 - accuracy: 0.6991 - val_loss: 7.4508 - val_accuracy: 0.1250\n",
            "Epoch 461/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9533 - accuracy: 0.6969 - val_loss: 8.3037 - val_accuracy: 0.1250\n",
            "Epoch 462/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9574 - accuracy: 0.6963 - val_loss: 7.7645 - val_accuracy: 0.1250\n",
            "Epoch 463/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9695 - accuracy: 0.6933 - val_loss: 7.3491 - val_accuracy: 0.1250\n",
            "Epoch 464/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9541 - accuracy: 0.6950 - val_loss: 7.8201 - val_accuracy: 0.1250\n",
            "Epoch 465/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9658 - accuracy: 0.6924 - val_loss: 7.6402 - val_accuracy: 0.1250\n",
            "Epoch 466/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9704 - accuracy: 0.6878 - val_loss: 8.1317 - val_accuracy: 0.1250\n",
            "Epoch 467/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9529 - accuracy: 0.6979 - val_loss: 7.8060 - val_accuracy: 0.1250\n",
            "Epoch 468/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9444 - accuracy: 0.6980 - val_loss: 7.7726 - val_accuracy: 0.1250\n",
            "Epoch 469/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9493 - accuracy: 0.6957 - val_loss: 7.5248 - val_accuracy: 0.1250\n",
            "Epoch 470/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9445 - accuracy: 0.6976 - val_loss: 7.6436 - val_accuracy: 0.1250\n",
            "Epoch 471/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9551 - accuracy: 0.6950 - val_loss: 7.7889 - val_accuracy: 0.1250\n",
            "Epoch 472/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9573 - accuracy: 0.6932 - val_loss: 7.2815 - val_accuracy: 0.1250\n",
            "Epoch 473/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9536 - accuracy: 0.6930 - val_loss: 7.5644 - val_accuracy: 0.1250\n",
            "Epoch 474/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9517 - accuracy: 0.6939 - val_loss: 7.8919 - val_accuracy: 0.1250\n",
            "Epoch 475/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9483 - accuracy: 0.6958 - val_loss: 7.9938 - val_accuracy: 0.1250\n",
            "Epoch 476/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9566 - accuracy: 0.7004 - val_loss: 7.5698 - val_accuracy: 0.1250\n",
            "Epoch 477/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9536 - accuracy: 0.6985 - val_loss: 7.7021 - val_accuracy: 0.1250\n",
            "Epoch 478/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9318 - accuracy: 0.7020 - val_loss: 7.6189 - val_accuracy: 0.1250\n",
            "Epoch 479/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9486 - accuracy: 0.6964 - val_loss: 7.5949 - val_accuracy: 0.1250\n",
            "Epoch 480/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9417 - accuracy: 0.6968 - val_loss: 7.8684 - val_accuracy: 0.1250\n",
            "Epoch 481/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9440 - accuracy: 0.7037 - val_loss: 7.8265 - val_accuracy: 0.1250\n",
            "Epoch 482/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9464 - accuracy: 0.6965 - val_loss: 7.8415 - val_accuracy: 0.1250\n",
            "Epoch 483/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9455 - accuracy: 0.6996 - val_loss: 7.7918 - val_accuracy: 0.1250\n",
            "Epoch 484/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9399 - accuracy: 0.7029 - val_loss: 7.9054 - val_accuracy: 0.1250\n",
            "Epoch 485/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9559 - accuracy: 0.6953 - val_loss: 8.0448 - val_accuracy: 0.1250\n",
            "Epoch 486/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9492 - accuracy: 0.6975 - val_loss: 7.9048 - val_accuracy: 0.1250\n",
            "Epoch 487/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9385 - accuracy: 0.6994 - val_loss: 7.9081 - val_accuracy: 0.1250\n",
            "Epoch 488/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9559 - accuracy: 0.6964 - val_loss: 7.7180 - val_accuracy: 0.1250\n",
            "Epoch 489/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9471 - accuracy: 0.6957 - val_loss: 7.7967 - val_accuracy: 0.1250\n",
            "Epoch 490/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9143 - accuracy: 0.7055 - val_loss: 8.0305 - val_accuracy: 0.1250\n",
            "Epoch 491/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9480 - accuracy: 0.6985 - val_loss: 7.4515 - val_accuracy: 0.1250\n",
            "Epoch 492/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9536 - accuracy: 0.6952 - val_loss: 7.6735 - val_accuracy: 0.1250\n",
            "Epoch 493/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9361 - accuracy: 0.7005 - val_loss: 7.7332 - val_accuracy: 0.1250\n",
            "Epoch 494/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9385 - accuracy: 0.6979 - val_loss: 7.8471 - val_accuracy: 0.1250\n",
            "Epoch 495/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9544 - accuracy: 0.6977 - val_loss: 7.6839 - val_accuracy: 0.1250\n",
            "Epoch 496/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9434 - accuracy: 0.6988 - val_loss: 7.6543 - val_accuracy: 0.1250\n",
            "Epoch 497/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9477 - accuracy: 0.6956 - val_loss: 8.0426 - val_accuracy: 0.1250\n",
            "Epoch 498/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9364 - accuracy: 0.6999 - val_loss: 8.3945 - val_accuracy: 0.1250\n",
            "Epoch 499/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9407 - accuracy: 0.7007 - val_loss: 8.2813 - val_accuracy: 0.1250\n",
            "Epoch 500/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9452 - accuracy: 0.7020 - val_loss: 8.0317 - val_accuracy: 0.1250\n",
            "Epoch 501/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9487 - accuracy: 0.6992 - val_loss: 7.7865 - val_accuracy: 0.1250\n",
            "Epoch 502/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9446 - accuracy: 0.6989 - val_loss: 8.0718 - val_accuracy: 0.1250\n",
            "Epoch 503/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9465 - accuracy: 0.6967 - val_loss: 7.7036 - val_accuracy: 0.1250\n",
            "Epoch 504/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9329 - accuracy: 0.7042 - val_loss: 7.8946 - val_accuracy: 0.1250\n",
            "Epoch 505/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9540 - accuracy: 0.6953 - val_loss: 8.3549 - val_accuracy: 0.1250\n",
            "Epoch 506/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9280 - accuracy: 0.7042 - val_loss: 7.9276 - val_accuracy: 0.1250\n",
            "Epoch 507/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9454 - accuracy: 0.6998 - val_loss: 8.2067 - val_accuracy: 0.1250\n",
            "Epoch 508/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9222 - accuracy: 0.7049 - val_loss: 8.4780 - val_accuracy: 0.1250\n",
            "Epoch 509/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9506 - accuracy: 0.6954 - val_loss: 7.9485 - val_accuracy: 0.1250\n",
            "Epoch 510/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9476 - accuracy: 0.7013 - val_loss: 8.4815 - val_accuracy: 0.1250\n",
            "Epoch 511/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9380 - accuracy: 0.7006 - val_loss: 8.0012 - val_accuracy: 0.1250\n",
            "Epoch 512/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9334 - accuracy: 0.7022 - val_loss: 8.2582 - val_accuracy: 0.1250\n",
            "Epoch 513/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9299 - accuracy: 0.7045 - val_loss: 8.2718 - val_accuracy: 0.1250\n",
            "Epoch 514/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9212 - accuracy: 0.7047 - val_loss: 8.2918 - val_accuracy: 0.1250\n",
            "Epoch 515/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9292 - accuracy: 0.7007 - val_loss: 8.4463 - val_accuracy: 0.1250\n",
            "Epoch 516/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9357 - accuracy: 0.7028 - val_loss: 8.4155 - val_accuracy: 0.1250\n",
            "Epoch 517/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9342 - accuracy: 0.6999 - val_loss: 8.3080 - val_accuracy: 0.1250\n",
            "Epoch 518/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9383 - accuracy: 0.7009 - val_loss: 8.3171 - val_accuracy: 0.1250\n",
            "Epoch 519/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9451 - accuracy: 0.7018 - val_loss: 8.4790 - val_accuracy: 0.1250\n",
            "Epoch 520/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9301 - accuracy: 0.7025 - val_loss: 8.4406 - val_accuracy: 0.1250\n",
            "Epoch 521/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9439 - accuracy: 0.7025 - val_loss: 8.5435 - val_accuracy: 0.1250\n",
            "Epoch 522/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9315 - accuracy: 0.7013 - val_loss: 8.3451 - val_accuracy: 0.1250\n",
            "Epoch 523/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.9302 - accuracy: 0.7039 - val_loss: 8.3268 - val_accuracy: 0.1250\n",
            "Epoch 524/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9342 - accuracy: 0.7005 - val_loss: 8.5045 - val_accuracy: 0.1250\n",
            "Epoch 525/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9363 - accuracy: 0.7000 - val_loss: 8.4510 - val_accuracy: 0.1250\n",
            "Epoch 526/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9244 - accuracy: 0.7041 - val_loss: 8.0772 - val_accuracy: 0.1250\n",
            "Epoch 527/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9434 - accuracy: 0.7020 - val_loss: 8.9879 - val_accuracy: 0.1250\n",
            "Epoch 528/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9189 - accuracy: 0.7085 - val_loss: 8.4958 - val_accuracy: 0.1250\n",
            "Epoch 529/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9307 - accuracy: 0.7025 - val_loss: 8.5766 - val_accuracy: 0.1250\n",
            "Epoch 530/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9238 - accuracy: 0.7027 - val_loss: 8.4386 - val_accuracy: 0.1250\n",
            "Epoch 531/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9309 - accuracy: 0.7079 - val_loss: 8.6457 - val_accuracy: 0.1250\n",
            "Epoch 532/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9277 - accuracy: 0.7041 - val_loss: 9.0666 - val_accuracy: 0.1250\n",
            "Epoch 533/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9141 - accuracy: 0.7069 - val_loss: 8.8395 - val_accuracy: 0.1250\n",
            "Epoch 534/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9381 - accuracy: 0.7000 - val_loss: 8.5105 - val_accuracy: 0.1250\n",
            "Epoch 535/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9156 - accuracy: 0.7079 - val_loss: 8.5665 - val_accuracy: 0.1250\n",
            "Epoch 536/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9181 - accuracy: 0.7051 - val_loss: 8.9030 - val_accuracy: 0.1250\n",
            "Epoch 537/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9267 - accuracy: 0.7053 - val_loss: 8.5690 - val_accuracy: 0.1250\n",
            "Epoch 538/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9214 - accuracy: 0.7025 - val_loss: 8.7374 - val_accuracy: 0.1250\n",
            "Epoch 539/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9347 - accuracy: 0.7010 - val_loss: 8.5842 - val_accuracy: 0.1250\n",
            "Epoch 540/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9237 - accuracy: 0.7074 - val_loss: 8.8274 - val_accuracy: 0.1250\n",
            "Epoch 541/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9169 - accuracy: 0.7098 - val_loss: 8.5420 - val_accuracy: 0.1250\n",
            "Epoch 542/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9255 - accuracy: 0.7045 - val_loss: 8.3699 - val_accuracy: 0.1250\n",
            "Epoch 543/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9178 - accuracy: 0.7082 - val_loss: 8.5412 - val_accuracy: 0.1250\n",
            "Epoch 544/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9142 - accuracy: 0.7063 - val_loss: 8.9415 - val_accuracy: 0.1250\n",
            "Epoch 545/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9235 - accuracy: 0.7067 - val_loss: 8.9490 - val_accuracy: 0.1250\n",
            "Epoch 546/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9291 - accuracy: 0.7063 - val_loss: 8.8800 - val_accuracy: 0.1250\n",
            "Epoch 547/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9173 - accuracy: 0.7054 - val_loss: 8.5935 - val_accuracy: 0.1250\n",
            "Epoch 548/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9436 - accuracy: 0.7000 - val_loss: 8.7022 - val_accuracy: 0.1250\n",
            "Epoch 549/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9223 - accuracy: 0.7050 - val_loss: 9.1985 - val_accuracy: 0.1250\n",
            "Epoch 550/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9205 - accuracy: 0.7100 - val_loss: 8.9146 - val_accuracy: 0.1250\n",
            "Epoch 551/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9105 - accuracy: 0.7075 - val_loss: 8.3628 - val_accuracy: 0.1250\n",
            "Epoch 552/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9258 - accuracy: 0.7047 - val_loss: 8.8422 - val_accuracy: 0.1250\n",
            "Epoch 553/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9182 - accuracy: 0.7058 - val_loss: 8.7292 - val_accuracy: 0.1250\n",
            "Epoch 554/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9243 - accuracy: 0.7068 - val_loss: 9.1216 - val_accuracy: 0.1250\n",
            "Epoch 555/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9082 - accuracy: 0.7069 - val_loss: 8.5102 - val_accuracy: 0.1250\n",
            "Epoch 556/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9062 - accuracy: 0.7120 - val_loss: 8.7764 - val_accuracy: 0.1250\n",
            "Epoch 557/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9177 - accuracy: 0.7101 - val_loss: 8.8324 - val_accuracy: 0.1250\n",
            "Epoch 558/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9241 - accuracy: 0.7080 - val_loss: 8.8310 - val_accuracy: 0.1250\n",
            "Epoch 559/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9000 - accuracy: 0.7124 - val_loss: 9.6001 - val_accuracy: 0.1250\n",
            "Epoch 560/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9192 - accuracy: 0.7076 - val_loss: 8.9405 - val_accuracy: 0.1250\n",
            "Epoch 561/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9189 - accuracy: 0.7062 - val_loss: 9.2280 - val_accuracy: 0.1250\n",
            "Epoch 562/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9121 - accuracy: 0.7125 - val_loss: 8.7715 - val_accuracy: 0.1250\n",
            "Epoch 563/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9152 - accuracy: 0.7103 - val_loss: 9.1266 - val_accuracy: 0.1250\n",
            "Epoch 564/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9115 - accuracy: 0.7104 - val_loss: 8.8472 - val_accuracy: 0.1250\n",
            "Epoch 565/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9124 - accuracy: 0.7088 - val_loss: 8.8654 - val_accuracy: 0.1250\n",
            "Epoch 566/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9050 - accuracy: 0.7086 - val_loss: 9.1295 - val_accuracy: 0.1250\n",
            "Epoch 567/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9218 - accuracy: 0.7101 - val_loss: 9.2063 - val_accuracy: 0.1250\n",
            "Epoch 568/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9259 - accuracy: 0.7031 - val_loss: 9.1520 - val_accuracy: 0.1250\n",
            "Epoch 569/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9050 - accuracy: 0.7147 - val_loss: 9.1218 - val_accuracy: 0.1250\n",
            "Epoch 570/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9140 - accuracy: 0.7084 - val_loss: 9.2999 - val_accuracy: 0.1250\n",
            "Epoch 571/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9112 - accuracy: 0.7093 - val_loss: 9.2252 - val_accuracy: 0.1250\n",
            "Epoch 572/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9131 - accuracy: 0.7100 - val_loss: 8.9775 - val_accuracy: 0.1250\n",
            "Epoch 573/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9009 - accuracy: 0.7115 - val_loss: 9.3718 - val_accuracy: 0.1250\n",
            "Epoch 574/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9079 - accuracy: 0.7088 - val_loss: 9.0786 - val_accuracy: 0.1250\n",
            "Epoch 575/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9179 - accuracy: 0.7097 - val_loss: 9.2654 - val_accuracy: 0.1250\n",
            "Epoch 576/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9066 - accuracy: 0.7109 - val_loss: 8.8629 - val_accuracy: 0.1250\n",
            "Epoch 577/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9089 - accuracy: 0.7114 - val_loss: 8.9784 - val_accuracy: 0.1250\n",
            "Epoch 578/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9028 - accuracy: 0.7088 - val_loss: 9.3872 - val_accuracy: 0.1250\n",
            "Epoch 579/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9164 - accuracy: 0.7061 - val_loss: 9.4493 - val_accuracy: 0.1250\n",
            "Epoch 580/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9091 - accuracy: 0.7112 - val_loss: 8.8046 - val_accuracy: 0.1250\n",
            "Epoch 581/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9035 - accuracy: 0.7127 - val_loss: 9.2780 - val_accuracy: 0.1250\n",
            "Epoch 582/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9034 - accuracy: 0.7118 - val_loss: 9.3660 - val_accuracy: 0.1250\n",
            "Epoch 583/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9123 - accuracy: 0.7086 - val_loss: 8.9974 - val_accuracy: 0.1250\n",
            "Epoch 584/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9057 - accuracy: 0.7122 - val_loss: 9.1119 - val_accuracy: 0.1250\n",
            "Epoch 585/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9095 - accuracy: 0.7100 - val_loss: 8.6803 - val_accuracy: 0.1250\n",
            "Epoch 586/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9117 - accuracy: 0.7097 - val_loss: 9.2264 - val_accuracy: 0.1250\n",
            "Epoch 587/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9073 - accuracy: 0.7126 - val_loss: 9.1554 - val_accuracy: 0.1250\n",
            "Epoch 588/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9111 - accuracy: 0.7076 - val_loss: 8.8717 - val_accuracy: 0.1250\n",
            "Epoch 589/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8915 - accuracy: 0.7139 - val_loss: 9.2788 - val_accuracy: 0.1250\n",
            "Epoch 590/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8968 - accuracy: 0.7102 - val_loss: 9.2996 - val_accuracy: 0.1250\n",
            "Epoch 591/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9158 - accuracy: 0.7120 - val_loss: 9.4887 - val_accuracy: 0.1250\n",
            "Epoch 592/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8910 - accuracy: 0.7143 - val_loss: 9.4650 - val_accuracy: 0.1250\n",
            "Epoch 593/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9001 - accuracy: 0.7114 - val_loss: 8.8735 - val_accuracy: 0.1250\n",
            "Epoch 594/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8923 - accuracy: 0.7133 - val_loss: 9.2109 - val_accuracy: 0.1250\n",
            "Epoch 595/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9085 - accuracy: 0.7100 - val_loss: 9.6553 - val_accuracy: 0.1250\n",
            "Epoch 596/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8935 - accuracy: 0.7168 - val_loss: 9.4138 - val_accuracy: 0.1250\n",
            "Epoch 597/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9002 - accuracy: 0.7158 - val_loss: 8.9827 - val_accuracy: 0.1250\n",
            "Epoch 598/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9053 - accuracy: 0.7131 - val_loss: 9.2025 - val_accuracy: 0.1250\n",
            "Epoch 599/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9083 - accuracy: 0.7112 - val_loss: 9.4782 - val_accuracy: 0.1250\n",
            "Epoch 600/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8961 - accuracy: 0.7119 - val_loss: 9.4214 - val_accuracy: 0.1250\n",
            "Epoch 601/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8959 - accuracy: 0.7141 - val_loss: 9.0627 - val_accuracy: 0.1250\n",
            "Epoch 602/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8849 - accuracy: 0.7147 - val_loss: 9.7252 - val_accuracy: 0.1250\n",
            "Epoch 603/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8986 - accuracy: 0.7133 - val_loss: 9.2794 - val_accuracy: 0.1250\n",
            "Epoch 604/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8926 - accuracy: 0.7173 - val_loss: 9.4912 - val_accuracy: 0.1250\n",
            "Epoch 605/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8972 - accuracy: 0.7143 - val_loss: 9.2696 - val_accuracy: 0.1250\n",
            "Epoch 606/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8926 - accuracy: 0.7145 - val_loss: 9.6282 - val_accuracy: 0.1250\n",
            "Epoch 607/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8994 - accuracy: 0.7125 - val_loss: 9.6330 - val_accuracy: 0.1250\n",
            "Epoch 608/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8993 - accuracy: 0.7148 - val_loss: 9.6087 - val_accuracy: 0.1250\n",
            "Epoch 609/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8973 - accuracy: 0.7128 - val_loss: 9.3738 - val_accuracy: 0.1250\n",
            "Epoch 610/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8866 - accuracy: 0.7201 - val_loss: 9.4151 - val_accuracy: 0.1250\n",
            "Epoch 611/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8919 - accuracy: 0.7120 - val_loss: 9.7884 - val_accuracy: 0.1250\n",
            "Epoch 612/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8825 - accuracy: 0.7164 - val_loss: 9.6214 - val_accuracy: 0.1250\n",
            "Epoch 613/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8952 - accuracy: 0.7145 - val_loss: 9.5342 - val_accuracy: 0.1250\n",
            "Epoch 614/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8791 - accuracy: 0.7188 - val_loss: 9.4596 - val_accuracy: 0.1250\n",
            "Epoch 615/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8783 - accuracy: 0.7202 - val_loss: 9.4278 - val_accuracy: 0.1250\n",
            "Epoch 616/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.9099 - accuracy: 0.7096 - val_loss: 9.4701 - val_accuracy: 0.1250\n",
            "Epoch 617/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8845 - accuracy: 0.7156 - val_loss: 9.2264 - val_accuracy: 0.1250\n",
            "Epoch 618/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8911 - accuracy: 0.7177 - val_loss: 9.5520 - val_accuracy: 0.1250\n",
            "Epoch 619/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8843 - accuracy: 0.7181 - val_loss: 9.3729 - val_accuracy: 0.1250\n",
            "Epoch 620/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8804 - accuracy: 0.7181 - val_loss: 9.5194 - val_accuracy: 0.1250\n",
            "Epoch 621/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8848 - accuracy: 0.7169 - val_loss: 9.6579 - val_accuracy: 0.1250\n",
            "Epoch 622/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8830 - accuracy: 0.7199 - val_loss: 9.1670 - val_accuracy: 0.1250\n",
            "Epoch 623/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8832 - accuracy: 0.7181 - val_loss: 8.9796 - val_accuracy: 0.1250\n",
            "Epoch 624/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8952 - accuracy: 0.7163 - val_loss: 9.4776 - val_accuracy: 0.1250\n",
            "Epoch 625/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8932 - accuracy: 0.7132 - val_loss: 9.4854 - val_accuracy: 0.1250\n",
            "Epoch 626/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8811 - accuracy: 0.7191 - val_loss: 8.9871 - val_accuracy: 0.1250\n",
            "Epoch 627/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8924 - accuracy: 0.7150 - val_loss: 9.5626 - val_accuracy: 0.1250\n",
            "Epoch 628/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8863 - accuracy: 0.7157 - val_loss: 9.8237 - val_accuracy: 0.1250\n",
            "Epoch 629/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8763 - accuracy: 0.7187 - val_loss: 9.0904 - val_accuracy: 0.1250\n",
            "Epoch 630/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8981 - accuracy: 0.7167 - val_loss: 9.2295 - val_accuracy: 0.1250\n",
            "Epoch 631/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8871 - accuracy: 0.7177 - val_loss: 9.1702 - val_accuracy: 0.1250\n",
            "Epoch 632/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8856 - accuracy: 0.7172 - val_loss: 9.5370 - val_accuracy: 0.1250\n",
            "Epoch 633/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8766 - accuracy: 0.7177 - val_loss: 9.6242 - val_accuracy: 0.1250\n",
            "Epoch 634/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8915 - accuracy: 0.7174 - val_loss: 9.8278 - val_accuracy: 0.1250\n",
            "Epoch 635/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8862 - accuracy: 0.7184 - val_loss: 9.7970 - val_accuracy: 0.1250\n",
            "Epoch 636/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8961 - accuracy: 0.7159 - val_loss: 9.8540 - val_accuracy: 0.1250\n",
            "Epoch 637/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8799 - accuracy: 0.7209 - val_loss: 9.8891 - val_accuracy: 0.1250\n",
            "Epoch 638/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8727 - accuracy: 0.7217 - val_loss: 9.9242 - val_accuracy: 0.1250\n",
            "Epoch 639/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8887 - accuracy: 0.7204 - val_loss: 9.7630 - val_accuracy: 0.1250\n",
            "Epoch 640/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8873 - accuracy: 0.7161 - val_loss: 9.9862 - val_accuracy: 0.1250\n",
            "Epoch 641/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8957 - accuracy: 0.7140 - val_loss: 9.9689 - val_accuracy: 0.1250\n",
            "Epoch 642/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8799 - accuracy: 0.7139 - val_loss: 9.9589 - val_accuracy: 0.1250\n",
            "Epoch 643/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8729 - accuracy: 0.7215 - val_loss: 9.8667 - val_accuracy: 0.1250\n",
            "Epoch 644/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8792 - accuracy: 0.7205 - val_loss: 9.6603 - val_accuracy: 0.1250\n",
            "Epoch 645/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8816 - accuracy: 0.7204 - val_loss: 9.5941 - val_accuracy: 0.1250\n",
            "Epoch 646/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8880 - accuracy: 0.7150 - val_loss: 9.8442 - val_accuracy: 0.1250\n",
            "Epoch 647/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8841 - accuracy: 0.7195 - val_loss: 9.6729 - val_accuracy: 0.1250\n",
            "Epoch 648/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8837 - accuracy: 0.7173 - val_loss: 9.7907 - val_accuracy: 0.1250\n",
            "Epoch 649/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8923 - accuracy: 0.7154 - val_loss: 9.4690 - val_accuracy: 0.1250\n",
            "Epoch 650/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8859 - accuracy: 0.7168 - val_loss: 9.7065 - val_accuracy: 0.1250\n",
            "Epoch 651/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8827 - accuracy: 0.7188 - val_loss: 9.4757 - val_accuracy: 0.1250\n",
            "Epoch 652/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8782 - accuracy: 0.7185 - val_loss: 9.6340 - val_accuracy: 0.1250\n",
            "Epoch 653/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8892 - accuracy: 0.7143 - val_loss: 9.6674 - val_accuracy: 0.1250\n",
            "Epoch 654/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8871 - accuracy: 0.7189 - val_loss: 9.8025 - val_accuracy: 0.1250\n",
            "Epoch 655/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8782 - accuracy: 0.7207 - val_loss: 9.8747 - val_accuracy: 0.1250\n",
            "Epoch 656/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8756 - accuracy: 0.7190 - val_loss: 9.9502 - val_accuracy: 0.1250\n",
            "Epoch 657/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8778 - accuracy: 0.7202 - val_loss: 10.0980 - val_accuracy: 0.1250\n",
            "Epoch 658/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8744 - accuracy: 0.7223 - val_loss: 9.4829 - val_accuracy: 0.1250\n",
            "Epoch 659/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8723 - accuracy: 0.7207 - val_loss: 9.5468 - val_accuracy: 0.1250\n",
            "Epoch 660/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8738 - accuracy: 0.7211 - val_loss: 9.4531 - val_accuracy: 0.1250\n",
            "Epoch 661/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8964 - accuracy: 0.7165 - val_loss: 9.7684 - val_accuracy: 0.1250\n",
            "Epoch 662/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8697 - accuracy: 0.7237 - val_loss: 9.5793 - val_accuracy: 0.1250\n",
            "Epoch 663/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8793 - accuracy: 0.7189 - val_loss: 9.5568 - val_accuracy: 0.1250\n",
            "Epoch 664/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8754 - accuracy: 0.7192 - val_loss: 9.6794 - val_accuracy: 0.1250\n",
            "Epoch 665/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8733 - accuracy: 0.7233 - val_loss: 9.8637 - val_accuracy: 0.1250\n",
            "Epoch 666/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8761 - accuracy: 0.7179 - val_loss: 9.4889 - val_accuracy: 0.1250\n",
            "Epoch 667/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8714 - accuracy: 0.7224 - val_loss: 9.4964 - val_accuracy: 0.1250\n",
            "Epoch 668/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8824 - accuracy: 0.7227 - val_loss: 9.8370 - val_accuracy: 0.1250\n",
            "Epoch 669/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8752 - accuracy: 0.7204 - val_loss: 9.9125 - val_accuracy: 0.1250\n",
            "Epoch 670/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8768 - accuracy: 0.7194 - val_loss: 10.1329 - val_accuracy: 0.1250\n",
            "Epoch 671/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8689 - accuracy: 0.7218 - val_loss: 9.8866 - val_accuracy: 0.1250\n",
            "Epoch 672/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8847 - accuracy: 0.7150 - val_loss: 10.1202 - val_accuracy: 0.1250\n",
            "Epoch 673/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8640 - accuracy: 0.7229 - val_loss: 9.9815 - val_accuracy: 0.1250\n",
            "Epoch 674/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8723 - accuracy: 0.7210 - val_loss: 9.9167 - val_accuracy: 0.1250\n",
            "Epoch 675/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8812 - accuracy: 0.7173 - val_loss: 9.2911 - val_accuracy: 0.1250\n",
            "Epoch 676/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8826 - accuracy: 0.7238 - val_loss: 10.1042 - val_accuracy: 0.1250\n",
            "Epoch 677/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8766 - accuracy: 0.7205 - val_loss: 9.8918 - val_accuracy: 0.1250\n",
            "Epoch 678/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8706 - accuracy: 0.7232 - val_loss: 9.8402 - val_accuracy: 0.1250\n",
            "Epoch 679/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8756 - accuracy: 0.7199 - val_loss: 9.8772 - val_accuracy: 0.1250\n",
            "Epoch 680/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8664 - accuracy: 0.7240 - val_loss: 10.0355 - val_accuracy: 0.1250\n",
            "Epoch 681/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8628 - accuracy: 0.7191 - val_loss: 10.1210 - val_accuracy: 0.1250\n",
            "Epoch 682/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8710 - accuracy: 0.7207 - val_loss: 9.9596 - val_accuracy: 0.1250\n",
            "Epoch 683/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8725 - accuracy: 0.7223 - val_loss: 10.2074 - val_accuracy: 0.1250\n",
            "Epoch 684/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8727 - accuracy: 0.7193 - val_loss: 10.0659 - val_accuracy: 0.1250\n",
            "Epoch 685/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8741 - accuracy: 0.7198 - val_loss: 10.1579 - val_accuracy: 0.1250\n",
            "Epoch 686/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8643 - accuracy: 0.7229 - val_loss: 10.6610 - val_accuracy: 0.1250\n",
            "Epoch 687/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8593 - accuracy: 0.7251 - val_loss: 9.8641 - val_accuracy: 0.1250\n",
            "Epoch 688/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8650 - accuracy: 0.7246 - val_loss: 9.9721 - val_accuracy: 0.1250\n",
            "Epoch 689/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8725 - accuracy: 0.7234 - val_loss: 10.0457 - val_accuracy: 0.1250\n",
            "Epoch 690/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8707 - accuracy: 0.7224 - val_loss: 10.0160 - val_accuracy: 0.1250\n",
            "Epoch 691/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8665 - accuracy: 0.7256 - val_loss: 9.9495 - val_accuracy: 0.1250\n",
            "Epoch 692/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8587 - accuracy: 0.7234 - val_loss: 9.5131 - val_accuracy: 0.1250\n",
            "Epoch 693/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8792 - accuracy: 0.7206 - val_loss: 10.4244 - val_accuracy: 0.1250\n",
            "Epoch 694/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8511 - accuracy: 0.7301 - val_loss: 9.8218 - val_accuracy: 0.1250\n",
            "Epoch 695/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8644 - accuracy: 0.7248 - val_loss: 10.3001 - val_accuracy: 0.1250\n",
            "Epoch 696/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8519 - accuracy: 0.7272 - val_loss: 10.2760 - val_accuracy: 0.1250\n",
            "Epoch 697/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8685 - accuracy: 0.7234 - val_loss: 10.1436 - val_accuracy: 0.1250\n",
            "Epoch 698/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8696 - accuracy: 0.7200 - val_loss: 10.2951 - val_accuracy: 0.1250\n",
            "Epoch 699/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8713 - accuracy: 0.7209 - val_loss: 10.4496 - val_accuracy: 0.1250\n",
            "Epoch 700/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8644 - accuracy: 0.7261 - val_loss: 10.4211 - val_accuracy: 0.1250\n",
            "Epoch 701/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8625 - accuracy: 0.7222 - val_loss: 9.8176 - val_accuracy: 0.1250\n",
            "Epoch 702/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8648 - accuracy: 0.7243 - val_loss: 10.1722 - val_accuracy: 0.1250\n",
            "Epoch 703/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8731 - accuracy: 0.7231 - val_loss: 10.2274 - val_accuracy: 0.1250\n",
            "Epoch 704/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8587 - accuracy: 0.7278 - val_loss: 10.0785 - val_accuracy: 0.1250\n",
            "Epoch 705/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8500 - accuracy: 0.7244 - val_loss: 9.9385 - val_accuracy: 0.1250\n",
            "Epoch 706/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8572 - accuracy: 0.7259 - val_loss: 9.8699 - val_accuracy: 0.1250\n",
            "Epoch 707/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8586 - accuracy: 0.7217 - val_loss: 10.2915 - val_accuracy: 0.1250\n",
            "Epoch 708/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8610 - accuracy: 0.7256 - val_loss: 10.3125 - val_accuracy: 0.1250\n",
            "Epoch 709/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8602 - accuracy: 0.7213 - val_loss: 10.0965 - val_accuracy: 0.1250\n",
            "Epoch 710/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8560 - accuracy: 0.7275 - val_loss: 10.4896 - val_accuracy: 0.1250\n",
            "Epoch 711/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8612 - accuracy: 0.7246 - val_loss: 10.5060 - val_accuracy: 0.1250\n",
            "Epoch 712/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8609 - accuracy: 0.7267 - val_loss: 10.4531 - val_accuracy: 0.1250\n",
            "Epoch 713/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8719 - accuracy: 0.7199 - val_loss: 10.1599 - val_accuracy: 0.1250\n",
            "Epoch 714/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8649 - accuracy: 0.7241 - val_loss: 10.0413 - val_accuracy: 0.1250\n",
            "Epoch 715/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8524 - accuracy: 0.7233 - val_loss: 10.3345 - val_accuracy: 0.1250\n",
            "Epoch 716/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8566 - accuracy: 0.7250 - val_loss: 10.6826 - val_accuracy: 0.1250\n",
            "Epoch 717/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8521 - accuracy: 0.7299 - val_loss: 10.4459 - val_accuracy: 0.1250\n",
            "Epoch 718/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8542 - accuracy: 0.7279 - val_loss: 10.2294 - val_accuracy: 0.1250\n",
            "Epoch 719/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8646 - accuracy: 0.7251 - val_loss: 10.2024 - val_accuracy: 0.1250\n",
            "Epoch 720/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8593 - accuracy: 0.7271 - val_loss: 10.6685 - val_accuracy: 0.1250\n",
            "Epoch 721/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8638 - accuracy: 0.7223 - val_loss: 9.9306 - val_accuracy: 0.1250\n",
            "Epoch 722/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8495 - accuracy: 0.7232 - val_loss: 10.3285 - val_accuracy: 0.1250\n",
            "Epoch 723/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8564 - accuracy: 0.7270 - val_loss: 10.1753 - val_accuracy: 0.1250\n",
            "Epoch 724/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8580 - accuracy: 0.7220 - val_loss: 10.4608 - val_accuracy: 0.1250\n",
            "Epoch 725/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8558 - accuracy: 0.7233 - val_loss: 10.2295 - val_accuracy: 0.1250\n",
            "Epoch 726/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8638 - accuracy: 0.7240 - val_loss: 10.4810 - val_accuracy: 0.1250\n",
            "Epoch 727/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8517 - accuracy: 0.7271 - val_loss: 10.6456 - val_accuracy: 0.1250\n",
            "Epoch 728/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8477 - accuracy: 0.7278 - val_loss: 10.2121 - val_accuracy: 0.1250\n",
            "Epoch 729/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8512 - accuracy: 0.7293 - val_loss: 10.3577 - val_accuracy: 0.1250\n",
            "Epoch 730/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8676 - accuracy: 0.7249 - val_loss: 10.7204 - val_accuracy: 0.1250\n",
            "Epoch 731/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8560 - accuracy: 0.7278 - val_loss: 9.8624 - val_accuracy: 0.1250\n",
            "Epoch 732/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8681 - accuracy: 0.7215 - val_loss: 10.8278 - val_accuracy: 0.1250\n",
            "Epoch 733/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8448 - accuracy: 0.7255 - val_loss: 10.2799 - val_accuracy: 0.1250\n",
            "Epoch 734/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8586 - accuracy: 0.7239 - val_loss: 10.4211 - val_accuracy: 0.1250\n",
            "Epoch 735/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8470 - accuracy: 0.7244 - val_loss: 10.1496 - val_accuracy: 0.1250\n",
            "Epoch 736/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8492 - accuracy: 0.7269 - val_loss: 10.8185 - val_accuracy: 0.1250\n",
            "Epoch 737/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8544 - accuracy: 0.7291 - val_loss: 10.6237 - val_accuracy: 0.1250\n",
            "Epoch 738/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8492 - accuracy: 0.7268 - val_loss: 10.6448 - val_accuracy: 0.1250\n",
            "Epoch 739/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8366 - accuracy: 0.7310 - val_loss: 10.5584 - val_accuracy: 0.1250\n",
            "Epoch 740/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8613 - accuracy: 0.7253 - val_loss: 10.6622 - val_accuracy: 0.1250\n",
            "Epoch 741/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8399 - accuracy: 0.7288 - val_loss: 10.5300 - val_accuracy: 0.1250\n",
            "Epoch 742/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8441 - accuracy: 0.7282 - val_loss: 10.6838 - val_accuracy: 0.1250\n",
            "Epoch 743/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8500 - accuracy: 0.7299 - val_loss: 10.2503 - val_accuracy: 0.1250\n",
            "Epoch 744/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8524 - accuracy: 0.7284 - val_loss: 11.0174 - val_accuracy: 0.1250\n",
            "Epoch 745/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8599 - accuracy: 0.7236 - val_loss: 10.6581 - val_accuracy: 0.1250\n",
            "Epoch 746/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8468 - accuracy: 0.7285 - val_loss: 9.9244 - val_accuracy: 0.1250\n",
            "Epoch 747/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8565 - accuracy: 0.7277 - val_loss: 10.3276 - val_accuracy: 0.1250\n",
            "Epoch 748/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8471 - accuracy: 0.7266 - val_loss: 10.7333 - val_accuracy: 0.1250\n",
            "Epoch 749/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8462 - accuracy: 0.7269 - val_loss: 11.1194 - val_accuracy: 0.1250\n",
            "Epoch 750/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8491 - accuracy: 0.7255 - val_loss: 10.6923 - val_accuracy: 0.1250\n",
            "Epoch 751/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8486 - accuracy: 0.7287 - val_loss: 10.8833 - val_accuracy: 0.1250\n",
            "Epoch 752/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8484 - accuracy: 0.7280 - val_loss: 10.5131 - val_accuracy: 0.1250\n",
            "Epoch 753/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8447 - accuracy: 0.7287 - val_loss: 10.3909 - val_accuracy: 0.1250\n",
            "Epoch 754/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8484 - accuracy: 0.7282 - val_loss: 10.3248 - val_accuracy: 0.1250\n",
            "Epoch 755/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8416 - accuracy: 0.7284 - val_loss: 10.5221 - val_accuracy: 0.1250\n",
            "Epoch 756/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8327 - accuracy: 0.7316 - val_loss: 10.9323 - val_accuracy: 0.1250\n",
            "Epoch 757/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8468 - accuracy: 0.7263 - val_loss: 11.1247 - val_accuracy: 0.1250\n",
            "Epoch 758/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8448 - accuracy: 0.7285 - val_loss: 10.2535 - val_accuracy: 0.1250\n",
            "Epoch 759/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8498 - accuracy: 0.7282 - val_loss: 10.7227 - val_accuracy: 0.1250\n",
            "Epoch 760/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8410 - accuracy: 0.7278 - val_loss: 10.5460 - val_accuracy: 0.1250\n",
            "Epoch 761/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8460 - accuracy: 0.7261 - val_loss: 11.0814 - val_accuracy: 0.1250\n",
            "Epoch 762/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8375 - accuracy: 0.7298 - val_loss: 10.5028 - val_accuracy: 0.1250\n",
            "Epoch 763/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8434 - accuracy: 0.7260 - val_loss: 11.0670 - val_accuracy: 0.1250\n",
            "Epoch 764/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8500 - accuracy: 0.7292 - val_loss: 10.9986 - val_accuracy: 0.1250\n",
            "Epoch 765/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8422 - accuracy: 0.7337 - val_loss: 10.9682 - val_accuracy: 0.1250\n",
            "Epoch 766/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8400 - accuracy: 0.7302 - val_loss: 10.7531 - val_accuracy: 0.1250\n",
            "Epoch 767/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8440 - accuracy: 0.7268 - val_loss: 10.5674 - val_accuracy: 0.1250\n",
            "Epoch 768/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8431 - accuracy: 0.7254 - val_loss: 10.7799 - val_accuracy: 0.1250\n",
            "Epoch 769/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8246 - accuracy: 0.7348 - val_loss: 10.6339 - val_accuracy: 0.1250\n",
            "Epoch 770/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8339 - accuracy: 0.7297 - val_loss: 10.4652 - val_accuracy: 0.1250\n",
            "Epoch 771/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8453 - accuracy: 0.7293 - val_loss: 10.2799 - val_accuracy: 0.1250\n",
            "Epoch 772/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8415 - accuracy: 0.7295 - val_loss: 11.0141 - val_accuracy: 0.1250\n",
            "Epoch 773/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8377 - accuracy: 0.7291 - val_loss: 10.5448 - val_accuracy: 0.1250\n",
            "Epoch 774/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8350 - accuracy: 0.7261 - val_loss: 10.5942 - val_accuracy: 0.1250\n",
            "Epoch 775/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8343 - accuracy: 0.7364 - val_loss: 10.6569 - val_accuracy: 0.1250\n",
            "Epoch 776/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8513 - accuracy: 0.7258 - val_loss: 10.5286 - val_accuracy: 0.1250\n",
            "Epoch 777/1000\n",
            "202/202 [==============================] - 1s 7ms/step - loss: 0.8373 - accuracy: 0.7329 - val_loss: 11.1203 - val_accuracy: 0.1250\n",
            "Epoch 778/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8349 - accuracy: 0.7304 - val_loss: 10.9776 - val_accuracy: 0.1250\n",
            "Epoch 779/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8363 - accuracy: 0.7308 - val_loss: 11.1576 - val_accuracy: 0.1250\n",
            "Epoch 780/1000\n",
            "202/202 [==============================] - 1s 7ms/step - loss: 0.8306 - accuracy: 0.7334 - val_loss: 10.8088 - val_accuracy: 0.1250\n",
            "Epoch 781/1000\n",
            "202/202 [==============================] - 1s 7ms/step - loss: 0.8440 - accuracy: 0.7316 - val_loss: 10.2747 - val_accuracy: 0.1250\n",
            "Epoch 782/1000\n",
            "202/202 [==============================] - 1s 7ms/step - loss: 0.8362 - accuracy: 0.7316 - val_loss: 10.7000 - val_accuracy: 0.1250\n",
            "Epoch 783/1000\n",
            "202/202 [==============================] - 1s 7ms/step - loss: 0.8255 - accuracy: 0.7322 - val_loss: 10.3352 - val_accuracy: 0.1250\n",
            "Epoch 784/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8266 - accuracy: 0.7326 - val_loss: 11.0278 - val_accuracy: 0.1250\n",
            "Epoch 785/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8409 - accuracy: 0.7305 - val_loss: 11.2787 - val_accuracy: 0.1250\n",
            "Epoch 786/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8195 - accuracy: 0.7343 - val_loss: 11.4703 - val_accuracy: 0.1250\n",
            "Epoch 787/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8419 - accuracy: 0.7259 - val_loss: 11.1326 - val_accuracy: 0.1250\n",
            "Epoch 788/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8092 - accuracy: 0.7365 - val_loss: 11.0377 - val_accuracy: 0.1250\n",
            "Epoch 789/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8351 - accuracy: 0.7306 - val_loss: 11.1546 - val_accuracy: 0.1250\n",
            "Epoch 790/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8418 - accuracy: 0.7304 - val_loss: 10.8162 - val_accuracy: 0.1250\n",
            "Epoch 791/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8197 - accuracy: 0.7374 - val_loss: 11.1870 - val_accuracy: 0.1250\n",
            "Epoch 792/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8389 - accuracy: 0.7298 - val_loss: 10.8297 - val_accuracy: 0.1250\n",
            "Epoch 793/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8251 - accuracy: 0.7345 - val_loss: 10.6687 - val_accuracy: 0.1250\n",
            "Epoch 794/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8348 - accuracy: 0.7332 - val_loss: 10.8193 - val_accuracy: 0.1250\n",
            "Epoch 795/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8339 - accuracy: 0.7326 - val_loss: 10.3112 - val_accuracy: 0.1250\n",
            "Epoch 796/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8248 - accuracy: 0.7345 - val_loss: 10.7359 - val_accuracy: 0.1250\n",
            "Epoch 797/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8256 - accuracy: 0.7331 - val_loss: 10.7029 - val_accuracy: 0.1250\n",
            "Epoch 798/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8399 - accuracy: 0.7339 - val_loss: 10.6083 - val_accuracy: 0.1250\n",
            "Epoch 799/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8248 - accuracy: 0.7323 - val_loss: 11.5675 - val_accuracy: 0.1250\n",
            "Epoch 800/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8273 - accuracy: 0.7341 - val_loss: 11.2456 - val_accuracy: 0.1250\n",
            "Epoch 801/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8184 - accuracy: 0.7367 - val_loss: 10.8308 - val_accuracy: 0.1250\n",
            "Epoch 802/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8290 - accuracy: 0.7336 - val_loss: 10.5266 - val_accuracy: 0.1250\n",
            "Epoch 803/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8186 - accuracy: 0.7326 - val_loss: 10.5438 - val_accuracy: 0.1250\n",
            "Epoch 804/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8210 - accuracy: 0.7329 - val_loss: 10.8685 - val_accuracy: 0.1250\n",
            "Epoch 805/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8281 - accuracy: 0.7349 - val_loss: 10.9671 - val_accuracy: 0.1250\n",
            "Epoch 806/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8341 - accuracy: 0.7278 - val_loss: 11.3966 - val_accuracy: 0.1250\n",
            "Epoch 807/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8264 - accuracy: 0.7328 - val_loss: 11.0993 - val_accuracy: 0.1250\n",
            "Epoch 808/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8264 - accuracy: 0.7348 - val_loss: 11.1021 - val_accuracy: 0.1250\n",
            "Epoch 809/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8257 - accuracy: 0.7334 - val_loss: 11.7940 - val_accuracy: 0.1250\n",
            "Epoch 810/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8278 - accuracy: 0.7331 - val_loss: 10.9102 - val_accuracy: 0.1250\n",
            "Epoch 811/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8325 - accuracy: 0.7320 - val_loss: 11.4109 - val_accuracy: 0.1250\n",
            "Epoch 812/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8196 - accuracy: 0.7329 - val_loss: 10.9712 - val_accuracy: 0.1250\n",
            "Epoch 813/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8165 - accuracy: 0.7358 - val_loss: 11.0776 - val_accuracy: 0.1250\n",
            "Epoch 814/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8193 - accuracy: 0.7332 - val_loss: 11.1833 - val_accuracy: 0.1250\n",
            "Epoch 815/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8318 - accuracy: 0.7347 - val_loss: 10.4342 - val_accuracy: 0.1250\n",
            "Epoch 816/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8283 - accuracy: 0.7327 - val_loss: 10.8091 - val_accuracy: 0.1250\n",
            "Epoch 817/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8347 - accuracy: 0.7350 - val_loss: 11.1134 - val_accuracy: 0.1250\n",
            "Epoch 818/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8237 - accuracy: 0.7339 - val_loss: 11.3244 - val_accuracy: 0.1250\n",
            "Epoch 819/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8245 - accuracy: 0.7302 - val_loss: 10.5695 - val_accuracy: 0.1250\n",
            "Epoch 820/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8207 - accuracy: 0.7350 - val_loss: 10.8999 - val_accuracy: 0.1250\n",
            "Epoch 821/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8304 - accuracy: 0.7299 - val_loss: 11.0270 - val_accuracy: 0.1250\n",
            "Epoch 822/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8220 - accuracy: 0.7341 - val_loss: 11.0897 - val_accuracy: 0.1250\n",
            "Epoch 823/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8216 - accuracy: 0.7375 - val_loss: 11.2165 - val_accuracy: 0.1250\n",
            "Epoch 824/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8223 - accuracy: 0.7367 - val_loss: 10.9785 - val_accuracy: 0.1250\n",
            "Epoch 825/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8235 - accuracy: 0.7380 - val_loss: 11.0624 - val_accuracy: 0.1250\n",
            "Epoch 826/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8177 - accuracy: 0.7384 - val_loss: 11.4258 - val_accuracy: 0.1250\n",
            "Epoch 827/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8303 - accuracy: 0.7337 - val_loss: 10.9198 - val_accuracy: 0.1250\n",
            "Epoch 828/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8208 - accuracy: 0.7369 - val_loss: 11.4505 - val_accuracy: 0.1250\n",
            "Epoch 829/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8188 - accuracy: 0.7385 - val_loss: 10.6394 - val_accuracy: 0.1250\n",
            "Epoch 830/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8269 - accuracy: 0.7362 - val_loss: 11.1467 - val_accuracy: 0.1250\n",
            "Epoch 831/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8221 - accuracy: 0.7334 - val_loss: 11.1962 - val_accuracy: 0.1250\n",
            "Epoch 832/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8171 - accuracy: 0.7358 - val_loss: 11.0743 - val_accuracy: 0.1250\n",
            "Epoch 833/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8203 - accuracy: 0.7354 - val_loss: 11.4802 - val_accuracy: 0.1250\n",
            "Epoch 834/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8120 - accuracy: 0.7387 - val_loss: 11.3742 - val_accuracy: 0.1250\n",
            "Epoch 835/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8128 - accuracy: 0.7400 - val_loss: 11.4347 - val_accuracy: 0.1250\n",
            "Epoch 836/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8270 - accuracy: 0.7369 - val_loss: 11.1996 - val_accuracy: 0.1250\n",
            "Epoch 837/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8275 - accuracy: 0.7334 - val_loss: 11.3994 - val_accuracy: 0.1250\n",
            "Epoch 838/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8238 - accuracy: 0.7353 - val_loss: 11.5202 - val_accuracy: 0.1250\n",
            "Epoch 839/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8123 - accuracy: 0.7416 - val_loss: 11.3025 - val_accuracy: 0.1250\n",
            "Epoch 840/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8153 - accuracy: 0.7353 - val_loss: 11.5598 - val_accuracy: 0.1250\n",
            "Epoch 841/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8239 - accuracy: 0.7360 - val_loss: 11.3418 - val_accuracy: 0.1250\n",
            "Epoch 842/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8214 - accuracy: 0.7356 - val_loss: 11.1908 - val_accuracy: 0.1250\n",
            "Epoch 843/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8257 - accuracy: 0.7363 - val_loss: 11.3736 - val_accuracy: 0.1250\n",
            "Epoch 844/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.7308 - val_loss: 11.0708 - val_accuracy: 0.1250\n",
            "Epoch 845/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8232 - accuracy: 0.7370 - val_loss: 12.0306 - val_accuracy: 0.1250\n",
            "Epoch 846/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8280 - accuracy: 0.7321 - val_loss: 11.6131 - val_accuracy: 0.1250\n",
            "Epoch 847/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8237 - accuracy: 0.7366 - val_loss: 11.2077 - val_accuracy: 0.1250\n",
            "Epoch 848/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8197 - accuracy: 0.7369 - val_loss: 11.9670 - val_accuracy: 0.1250\n",
            "Epoch 849/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8132 - accuracy: 0.7387 - val_loss: 11.3970 - val_accuracy: 0.1250\n",
            "Epoch 850/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8182 - accuracy: 0.7320 - val_loss: 10.7646 - val_accuracy: 0.1250\n",
            "Epoch 851/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8212 - accuracy: 0.7346 - val_loss: 11.6114 - val_accuracy: 0.1250\n",
            "Epoch 852/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8171 - accuracy: 0.7379 - val_loss: 11.3379 - val_accuracy: 0.1250\n",
            "Epoch 853/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8255 - accuracy: 0.7357 - val_loss: 11.8172 - val_accuracy: 0.1250\n",
            "Epoch 854/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8272 - accuracy: 0.7352 - val_loss: 11.4683 - val_accuracy: 0.1250\n",
            "Epoch 855/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8212 - accuracy: 0.7372 - val_loss: 11.2071 - val_accuracy: 0.1250\n",
            "Epoch 856/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8224 - accuracy: 0.7352 - val_loss: 11.0641 - val_accuracy: 0.1250\n",
            "Epoch 857/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8201 - accuracy: 0.7330 - val_loss: 11.5770 - val_accuracy: 0.1250\n",
            "Epoch 858/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8057 - accuracy: 0.7400 - val_loss: 11.7061 - val_accuracy: 0.1250\n",
            "Epoch 859/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8146 - accuracy: 0.7358 - val_loss: 11.7946 - val_accuracy: 0.1250\n",
            "Epoch 860/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8128 - accuracy: 0.7388 - val_loss: 11.4744 - val_accuracy: 0.1250\n",
            "Epoch 861/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8153 - accuracy: 0.7397 - val_loss: 11.8823 - val_accuracy: 0.1250\n",
            "Epoch 862/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8135 - accuracy: 0.7355 - val_loss: 11.3488 - val_accuracy: 0.1250\n",
            "Epoch 863/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.7327 - val_loss: 11.8085 - val_accuracy: 0.1250\n",
            "Epoch 864/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8244 - accuracy: 0.7319 - val_loss: 11.8170 - val_accuracy: 0.1250\n",
            "Epoch 865/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8217 - accuracy: 0.7378 - val_loss: 11.2163 - val_accuracy: 0.1250\n",
            "Epoch 866/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8081 - accuracy: 0.7365 - val_loss: 11.6986 - val_accuracy: 0.1250\n",
            "Epoch 867/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8148 - accuracy: 0.7386 - val_loss: 11.8347 - val_accuracy: 0.1250\n",
            "Epoch 868/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8221 - accuracy: 0.7351 - val_loss: 12.2908 - val_accuracy: 0.1250\n",
            "Epoch 869/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8299 - accuracy: 0.7353 - val_loss: 11.3292 - val_accuracy: 0.1250\n",
            "Epoch 870/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8319 - accuracy: 0.7361 - val_loss: 12.0370 - val_accuracy: 0.1250\n",
            "Epoch 871/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8079 - accuracy: 0.7370 - val_loss: 11.8991 - val_accuracy: 0.1250\n",
            "Epoch 872/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8219 - accuracy: 0.7378 - val_loss: 12.2709 - val_accuracy: 0.1250\n",
            "Epoch 873/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8115 - accuracy: 0.7325 - val_loss: 11.8194 - val_accuracy: 0.1250\n",
            "Epoch 874/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8165 - accuracy: 0.7343 - val_loss: 11.7870 - val_accuracy: 0.1250\n",
            "Epoch 875/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.7402 - val_loss: 11.6568 - val_accuracy: 0.1250\n",
            "Epoch 876/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8122 - accuracy: 0.7407 - val_loss: 11.9176 - val_accuracy: 0.1250\n",
            "Epoch 877/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8177 - accuracy: 0.7366 - val_loss: 12.0052 - val_accuracy: 0.1250\n",
            "Epoch 878/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7973 - accuracy: 0.7439 - val_loss: 11.3223 - val_accuracy: 0.1250\n",
            "Epoch 879/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8136 - accuracy: 0.7388 - val_loss: 11.3957 - val_accuracy: 0.1250\n",
            "Epoch 880/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8181 - accuracy: 0.7385 - val_loss: 11.7134 - val_accuracy: 0.1250\n",
            "Epoch 881/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8186 - accuracy: 0.7389 - val_loss: 11.6544 - val_accuracy: 0.1250\n",
            "Epoch 882/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8091 - accuracy: 0.7394 - val_loss: 12.2566 - val_accuracy: 0.1250\n",
            "Epoch 883/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8060 - accuracy: 0.7396 - val_loss: 11.9129 - val_accuracy: 0.1250\n",
            "Epoch 884/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8273 - accuracy: 0.7367 - val_loss: 12.0134 - val_accuracy: 0.1250\n",
            "Epoch 885/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8146 - accuracy: 0.7402 - val_loss: 12.3683 - val_accuracy: 0.1250\n",
            "Epoch 886/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8255 - accuracy: 0.7338 - val_loss: 12.3579 - val_accuracy: 0.1250\n",
            "Epoch 887/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8092 - accuracy: 0.7420 - val_loss: 11.4572 - val_accuracy: 0.1250\n",
            "Epoch 888/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8148 - accuracy: 0.7381 - val_loss: 11.6829 - val_accuracy: 0.1250\n",
            "Epoch 889/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8190 - accuracy: 0.7350 - val_loss: 11.6819 - val_accuracy: 0.1250\n",
            "Epoch 890/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8164 - accuracy: 0.7379 - val_loss: 12.1953 - val_accuracy: 0.1250\n",
            "Epoch 891/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8108 - accuracy: 0.7401 - val_loss: 12.1631 - val_accuracy: 0.1250\n",
            "Epoch 892/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8172 - accuracy: 0.7375 - val_loss: 12.6137 - val_accuracy: 0.1250\n",
            "Epoch 893/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8124 - accuracy: 0.7382 - val_loss: 12.2510 - val_accuracy: 0.1250\n",
            "Epoch 894/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8177 - accuracy: 0.7344 - val_loss: 11.9761 - val_accuracy: 0.1250\n",
            "Epoch 895/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7964 - accuracy: 0.7446 - val_loss: 11.7033 - val_accuracy: 0.1250\n",
            "Epoch 896/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8171 - accuracy: 0.7379 - val_loss: 11.9415 - val_accuracy: 0.1250\n",
            "Epoch 897/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8183 - accuracy: 0.7353 - val_loss: 12.4264 - val_accuracy: 0.1250\n",
            "Epoch 898/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8159 - accuracy: 0.7410 - val_loss: 11.7309 - val_accuracy: 0.1250\n",
            "Epoch 899/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8037 - accuracy: 0.7423 - val_loss: 12.1238 - val_accuracy: 0.1250\n",
            "Epoch 900/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8112 - accuracy: 0.7398 - val_loss: 11.5652 - val_accuracy: 0.1250\n",
            "Epoch 901/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8147 - accuracy: 0.7411 - val_loss: 12.4631 - val_accuracy: 0.1250\n",
            "Epoch 902/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8167 - accuracy: 0.7381 - val_loss: 12.0793 - val_accuracy: 0.1250\n",
            "Epoch 903/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7989 - accuracy: 0.7378 - val_loss: 11.9179 - val_accuracy: 0.1250\n",
            "Epoch 904/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8183 - accuracy: 0.7342 - val_loss: 12.7777 - val_accuracy: 0.1250\n",
            "Epoch 905/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8186 - accuracy: 0.7370 - val_loss: 12.6000 - val_accuracy: 0.1250\n",
            "Epoch 906/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8117 - accuracy: 0.7378 - val_loss: 12.4780 - val_accuracy: 0.1250\n",
            "Epoch 907/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8195 - accuracy: 0.7363 - val_loss: 12.3611 - val_accuracy: 0.1250\n",
            "Epoch 908/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8052 - accuracy: 0.7402 - val_loss: 12.4382 - val_accuracy: 0.1250\n",
            "Epoch 909/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8031 - accuracy: 0.7431 - val_loss: 12.8850 - val_accuracy: 0.1250\n",
            "Epoch 910/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7997 - accuracy: 0.7431 - val_loss: 12.5769 - val_accuracy: 0.1250\n",
            "Epoch 911/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8258 - accuracy: 0.7342 - val_loss: 12.6867 - val_accuracy: 0.1250\n",
            "Epoch 912/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8099 - accuracy: 0.7403 - val_loss: 12.8211 - val_accuracy: 0.1250\n",
            "Epoch 913/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8160 - accuracy: 0.7339 - val_loss: 12.8291 - val_accuracy: 0.1250\n",
            "Epoch 914/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7993 - accuracy: 0.7418 - val_loss: 12.5288 - val_accuracy: 0.1250\n",
            "Epoch 915/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8213 - accuracy: 0.7375 - val_loss: 12.3312 - val_accuracy: 0.1250\n",
            "Epoch 916/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8110 - accuracy: 0.7376 - val_loss: 13.0876 - val_accuracy: 0.1250\n",
            "Epoch 917/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8145 - accuracy: 0.7385 - val_loss: 12.0831 - val_accuracy: 0.1250\n",
            "Epoch 918/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8111 - accuracy: 0.7394 - val_loss: 12.7108 - val_accuracy: 0.1250\n",
            "Epoch 919/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8053 - accuracy: 0.7416 - val_loss: 12.4063 - val_accuracy: 0.1250\n",
            "Epoch 920/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7887 - accuracy: 0.7453 - val_loss: 12.4166 - val_accuracy: 0.1250\n",
            "Epoch 921/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8194 - accuracy: 0.7362 - val_loss: 13.1867 - val_accuracy: 0.1250\n",
            "Epoch 922/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8115 - accuracy: 0.7424 - val_loss: 12.4250 - val_accuracy: 0.1250\n",
            "Epoch 923/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7999 - accuracy: 0.7381 - val_loss: 12.8439 - val_accuracy: 0.1250\n",
            "Epoch 924/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8041 - accuracy: 0.7401 - val_loss: 12.3573 - val_accuracy: 0.1250\n",
            "Epoch 925/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8048 - accuracy: 0.7420 - val_loss: 12.6967 - val_accuracy: 0.1250\n",
            "Epoch 926/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8187 - accuracy: 0.7378 - val_loss: 13.0473 - val_accuracy: 0.1250\n",
            "Epoch 927/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7985 - accuracy: 0.7412 - val_loss: 12.4903 - val_accuracy: 0.1250\n",
            "Epoch 928/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8064 - accuracy: 0.7429 - val_loss: 12.0556 - val_accuracy: 0.1250\n",
            "Epoch 929/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7972 - accuracy: 0.7438 - val_loss: 12.9328 - val_accuracy: 0.1250\n",
            "Epoch 930/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8047 - accuracy: 0.7411 - val_loss: 12.6565 - val_accuracy: 0.1250\n",
            "Epoch 931/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8073 - accuracy: 0.7384 - val_loss: 12.8538 - val_accuracy: 0.1250\n",
            "Epoch 932/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7983 - accuracy: 0.7435 - val_loss: 12.9857 - val_accuracy: 0.1250\n",
            "Epoch 933/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8166 - accuracy: 0.7378 - val_loss: 12.4070 - val_accuracy: 0.1250\n",
            "Epoch 934/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8019 - accuracy: 0.7438 - val_loss: 12.7187 - val_accuracy: 0.1250\n",
            "Epoch 935/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7945 - accuracy: 0.7446 - val_loss: 12.2535 - val_accuracy: 0.1250\n",
            "Epoch 936/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8046 - accuracy: 0.7438 - val_loss: 12.5211 - val_accuracy: 0.1250\n",
            "Epoch 937/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8014 - accuracy: 0.7435 - val_loss: 12.0553 - val_accuracy: 0.1250\n",
            "Epoch 938/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8037 - accuracy: 0.7400 - val_loss: 12.1883 - val_accuracy: 0.1250\n",
            "Epoch 939/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8075 - accuracy: 0.7395 - val_loss: 13.0068 - val_accuracy: 0.1250\n",
            "Epoch 940/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8073 - accuracy: 0.7381 - val_loss: 12.3325 - val_accuracy: 0.1250\n",
            "Epoch 941/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8078 - accuracy: 0.7419 - val_loss: 12.8228 - val_accuracy: 0.1250\n",
            "Epoch 942/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8013 - accuracy: 0.7440 - val_loss: 12.6598 - val_accuracy: 0.1250\n",
            "Epoch 943/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8072 - accuracy: 0.7405 - val_loss: 12.6734 - val_accuracy: 0.1250\n",
            "Epoch 944/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8036 - accuracy: 0.7407 - val_loss: 13.3102 - val_accuracy: 0.1250\n",
            "Epoch 945/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8036 - accuracy: 0.7403 - val_loss: 13.2521 - val_accuracy: 0.1250\n",
            "Epoch 946/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8115 - accuracy: 0.7404 - val_loss: 13.1512 - val_accuracy: 0.1250\n",
            "Epoch 947/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.7975 - accuracy: 0.7391 - val_loss: 12.5623 - val_accuracy: 0.1250\n",
            "Epoch 948/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8050 - accuracy: 0.7412 - val_loss: 13.0921 - val_accuracy: 0.1250\n",
            "Epoch 949/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8014 - accuracy: 0.7437 - val_loss: 13.0408 - val_accuracy: 0.1250\n",
            "Epoch 950/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8133 - accuracy: 0.7409 - val_loss: 13.4925 - val_accuracy: 0.1250\n",
            "Epoch 951/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8093 - accuracy: 0.7381 - val_loss: 12.9879 - val_accuracy: 0.1250\n",
            "Epoch 952/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8196 - accuracy: 0.7378 - val_loss: 12.7730 - val_accuracy: 0.1250\n",
            "Epoch 953/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8073 - accuracy: 0.7387 - val_loss: 12.8307 - val_accuracy: 0.1250\n",
            "Epoch 954/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8023 - accuracy: 0.7382 - val_loss: 12.7231 - val_accuracy: 0.1250\n",
            "Epoch 955/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7949 - accuracy: 0.7455 - val_loss: 12.7655 - val_accuracy: 0.1250\n",
            "Epoch 956/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7961 - accuracy: 0.7432 - val_loss: 13.6132 - val_accuracy: 0.1250\n",
            "Epoch 957/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.7371 - val_loss: 13.6859 - val_accuracy: 0.1250\n",
            "Epoch 958/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8053 - accuracy: 0.7423 - val_loss: 12.5939 - val_accuracy: 0.1250\n",
            "Epoch 959/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7957 - accuracy: 0.7430 - val_loss: 12.7971 - val_accuracy: 0.1250\n",
            "Epoch 960/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7950 - accuracy: 0.7425 - val_loss: 13.3208 - val_accuracy: 0.1250\n",
            "Epoch 961/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8055 - accuracy: 0.7432 - val_loss: 12.7526 - val_accuracy: 0.1250\n",
            "Epoch 962/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8057 - accuracy: 0.7380 - val_loss: 13.3365 - val_accuracy: 0.1250\n",
            "Epoch 963/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7929 - accuracy: 0.7457 - val_loss: 12.8495 - val_accuracy: 0.1250\n",
            "Epoch 964/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8044 - accuracy: 0.7435 - val_loss: 13.0615 - val_accuracy: 0.1250\n",
            "Epoch 965/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7982 - accuracy: 0.7442 - val_loss: 13.1081 - val_accuracy: 0.1250\n",
            "Epoch 966/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7995 - accuracy: 0.7411 - val_loss: 13.0448 - val_accuracy: 0.1250\n",
            "Epoch 967/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8017 - accuracy: 0.7424 - val_loss: 14.0738 - val_accuracy: 0.1250\n",
            "Epoch 968/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8022 - accuracy: 0.7461 - val_loss: 12.9217 - val_accuracy: 0.1250\n",
            "Epoch 969/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8028 - accuracy: 0.7438 - val_loss: 13.4393 - val_accuracy: 0.1250\n",
            "Epoch 970/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.7402 - val_loss: 13.1181 - val_accuracy: 0.1250\n",
            "Epoch 971/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8163 - accuracy: 0.7392 - val_loss: 13.9931 - val_accuracy: 0.1250\n",
            "Epoch 972/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7966 - accuracy: 0.7430 - val_loss: 13.1148 - val_accuracy: 0.1250\n",
            "Epoch 973/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7848 - accuracy: 0.7452 - val_loss: 13.6240 - val_accuracy: 0.0000e+00\n",
            "Epoch 974/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8016 - accuracy: 0.7385 - val_loss: 13.3578 - val_accuracy: 0.1250\n",
            "Epoch 975/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.8024 - accuracy: 0.7459 - val_loss: 13.7732 - val_accuracy: 0.1250\n",
            "Epoch 976/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7946 - accuracy: 0.7434 - val_loss: 13.3643 - val_accuracy: 0.1250\n",
            "Epoch 977/1000\n",
            "202/202 [==============================] - 1s 6ms/step - loss: 0.8097 - accuracy: 0.7398 - val_loss: 13.1653 - val_accuracy: 0.1250\n",
            "Epoch 978/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7950 - accuracy: 0.7448 - val_loss: 13.4711 - val_accuracy: 0.1250\n",
            "Epoch 979/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8000 - accuracy: 0.7404 - val_loss: 13.7166 - val_accuracy: 0.0000e+00\n",
            "Epoch 980/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7935 - accuracy: 0.7431 - val_loss: 13.7362 - val_accuracy: 0.1250\n",
            "Epoch 981/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7803 - accuracy: 0.7496 - val_loss: 13.7218 - val_accuracy: 0.1250\n",
            "Epoch 982/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7918 - accuracy: 0.7457 - val_loss: 13.2192 - val_accuracy: 0.1250\n",
            "Epoch 983/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8052 - accuracy: 0.7391 - val_loss: 13.2421 - val_accuracy: 0.1250\n",
            "Epoch 984/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7987 - accuracy: 0.7409 - val_loss: 12.8787 - val_accuracy: 0.1250\n",
            "Epoch 985/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7970 - accuracy: 0.7448 - val_loss: 13.3591 - val_accuracy: 0.1250\n",
            "Epoch 986/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7903 - accuracy: 0.7466 - val_loss: 13.4072 - val_accuracy: 0.1250\n",
            "Epoch 987/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7937 - accuracy: 0.7427 - val_loss: 12.4251 - val_accuracy: 0.1250\n",
            "Epoch 988/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7922 - accuracy: 0.7452 - val_loss: 13.7009 - val_accuracy: 0.1250\n",
            "Epoch 989/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.7438 - val_loss: 13.1613 - val_accuracy: 0.1250\n",
            "Epoch 990/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7900 - accuracy: 0.7466 - val_loss: 13.3177 - val_accuracy: 0.1250\n",
            "Epoch 991/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7938 - accuracy: 0.7451 - val_loss: 13.2728 - val_accuracy: 0.1250\n",
            "Epoch 992/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7941 - accuracy: 0.7434 - val_loss: 13.3541 - val_accuracy: 0.1250\n",
            "Epoch 993/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7834 - accuracy: 0.7465 - val_loss: 13.1877 - val_accuracy: 0.1250\n",
            "Epoch 994/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.7905 - accuracy: 0.7492 - val_loss: 13.7375 - val_accuracy: 0.1250\n",
            "Epoch 995/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7858 - accuracy: 0.7443 - val_loss: 13.4897 - val_accuracy: 0.1250\n",
            "Epoch 996/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7920 - accuracy: 0.7477 - val_loss: 13.4338 - val_accuracy: 0.1250\n",
            "Epoch 997/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.8050 - accuracy: 0.7395 - val_loss: 14.4668 - val_accuracy: 0.1250\n",
            "Epoch 998/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7840 - accuracy: 0.7497 - val_loss: 13.4486 - val_accuracy: 0.1250\n",
            "Epoch 999/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7906 - accuracy: 0.7456 - val_loss: 13.4949 - val_accuracy: 0.0000e+00\n",
            "Epoch 1000/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.7881 - accuracy: 0.7458 - val_loss: 14.0083 - val_accuracy: 0.1250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c320736d0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/training\"\n",
        "tfjs.converters.save_keras_model(model, checkpoint_path)"
      ],
      "metadata": {
        "id": "g75Y1ei2AhSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/training/asl_checkpoint')\n"
      ],
      "metadata": {
        "id": "m5wxtCfTEUDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('ASLmodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4F70xQBHZe1",
        "outputId": "a7706f5b-8139-4c6e-ccf9-4786bc7ffbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CY723GENkhL",
        "outputId": "4dd76be2-46d4-4127-ddcf-1c8029e89abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 14.0083 - accuracy: 0.1250\n",
            "[14.008272171020508, 0.125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "SrmHRYB4WQ7x",
        "outputId": "ae2bdc55-2057-41d0-9982-e91e59049399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-6e6b7aafa9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
          ]
        }
      ]
    }
  ]
}